<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Retrieval stage recommender systems with TensorFlow | Andres Blog</title>
<meta name="keywords" content="ML, TensorFlow, Recommender">
<meta name="description" content="Introduction to recommender systems Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations.">
<meta name="author" content="">
<link rel="canonical" href="https://andresruizc.github.io/andresr/posts/retrieval_tf/">
<link crossorigin="anonymous" href="/andresr/assets/css/stylesheet.265fc71454ed0df4c28557418d1be5cdf144d1b57d27561fa90e51d4bd689dbe.css" integrity="sha256-Jl/HFFTtDfTChVdBjRvlzfFE0bV9J1YfqQ5R1L1onb4=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/andresr/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://andresruizc.github.io/andresr/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://andresruizc.github.io/andresr/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://andresruizc.github.io/andresr/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://andresruizc.github.io/andresr/apple-touch-icon.png">
<link rel="mask-icon" href="https://andresruizc.github.io/andresr/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Retrieval stage recommender systems with TensorFlow" />
<meta property="og:description" content="Introduction to recommender systems Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://andresruizc.github.io/andresr/posts/retrieval_tf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-11T22:23:18+01:00" />
<meta property="article:modified_time" content="2023-02-11T22:23:18+01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Retrieval stage recommender systems with TensorFlow"/>
<meta name="twitter:description" content="Introduction to recommender systems Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://andresruizc.github.io/andresr/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Retrieval stage recommender systems with TensorFlow",
      "item": "https://andresruizc.github.io/andresr/posts/retrieval_tf/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Retrieval stage recommender systems with TensorFlow",
  "name": "Retrieval stage recommender systems with TensorFlow",
  "description": "Introduction to recommender systems Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations.",
  "keywords": [
    "ML", "TensorFlow", "Recommender"
  ],
  "articleBody": "Introduction to recommender systems Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations.\nHere are some types of recommender systems:\nCollaborative Filtering: Assumes people with similar preferences will have similar preferences in the future. If the person one like items A and B and person 2 likes item A, person two will likely want item B. Content-Based Filtering: Assumes people will prefer items like those they liked in the past. Suppose I bought a pair of shoes. Hybrid Recommender Systems: Combines both collaborative and content-based filtering. Matrix Factorization: Assumes hidden relationships in the data containing both users and items may exist. Association Rule Mining: Assumes items frequently purchased together are likely to interest the user. It is very common to work with vast amounts of data when building recommender systems, and computational and time requirements must be considered. That’s why real-world recommender systems are often divided into two stages:\nThe retrieval stage filters an initial set of potential candidates out of all possible ones. It must be computationally efficient because candidates are in the order of millions. The ranking stage uses the outputs of the retrieval model and selects the most likely candidates. In this tutorial, I will focus on the retrieval stage. Specifically, I will build a retrieval stage recommender system that predicts a set of movies a given user is likely to watch using TensorFlow and the MovieLens dataset.\nDataset The MovieLens dataset contains 100,000 ratings and 3,600 tags applied to 9,000 movies by 600 users, and it can be downloaded directly from TensorFlow Datasets. Both the datasets containing the movies and the rating will be downloaded, but we will only focus on the movies dataset.\nLet’s load both datasets and visualize the attributes of each observation.\n# Package installation !pip install -q tensorflow-recommenders !pip install -q --upgrade tensorflow-datasets !pip install -q scann # Loading libraries import os import pprint import tempfile from typing import Dict, Text import numpy as np import tensorflow as tf import tensorflow_datasets as tfds import tensorflow_recommenders as tfrs Let’s load both datasets\n# Ratings data. ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\") # Features of all the available movies. movies = tfds.load(\"movielens/100k-movies\", split=\"train\") And visualize their attributes\nfor x in ratings.take(1).as_numpy_iterator(): pprint.pprint(x) for x in movies.take(1).as_numpy_iterator(): pprint.pprint(x) Rating dataset: {'bucketized_user_age': 45.0, 'movie_genres': array([7]), 'movie_id': b'357', 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\", 'raw_user_age': 46.0, 'timestamp': 879024327, 'user_gender': True, 'user_id': b'138', 'user_occupation_label': 4, 'user_occupation_text': b'doctor', 'user_rating': 4.0, 'user_zip_code': b'53211'} Movie dataset: {'movie_genres': array([4]), 'movie_id': b'1681', 'movie_title': b'You So Crazy (1994)'} We only need the user_id and movie_title fields to build our retrieval stage. We are using TensorFlow for the data preprocessing stages because TensorFlow uses a specific data storage and message format that is targeted for ML workloads.Still, we could use other frameworks, such as Pandas, and then convert them back to a suitable form.\ndata storage and message format that is targeted for ML workloads.\nratings = ratings.map(lambda x: { \"movie_title\": x[\"movie_title\"], \"user_id\": x[\"user_id\"], }) movies = movies.map(lambda x: x[\"movie_title\"]) Training and test sets Why do we need to split our data into training and test sets? Because one of the most critical aspects of ML models is their ability to generalize well on unseen instances. If we were to use all our data to train the model and then evaluate model performance, we would not be capable of assessing how good our model is. Therefore, we need a training and test set.\nA good rule of thumb is to divide 80% for training and 20% for testing, and we’ll stick to that, but it depends on the specifics of your problem.\nThe strategy we use to split our data is also essential. If we are working with time series, we would have to divide based on timestamps. And the same applies to industrial recommender systems. In this tutorial, we won’t use any of that because our problem is simple.\ntf.random.set_seed(42) # to get reproducible partitions shuffled = ratings.shuffle(100000, seed=42, reshuffle_each_iteration=False) train = shuffled.take(80000) # 80% test = shuffled.skip(80000).take(20000) # 20% We also need to create lookup tables with unique values for our users and movie titles so that we can later use embeddings.\nmovie_titles = movies.batch(1000) user_ids = ratings.batch(1000000).map(lambda x: x[\"user_id\"]) unique_movie_titles = np.unique(np.concatenate(list(movie_titles))) unique_user_ids = np.unique(np.concatenate(list(user_ids))) Embeddings Now that we have a lookup table with unique values for our users and movie titles, we can use embeddings. Embeddings are dense representations of high-dimensional data, such as words or vocabulary. They can be understood as a lookup table that maps integer indices (e.g., user id and movie titles) to dense vectors.\nEmbeddings usually are initialized randomly, then trained using backpropagation along with the rest of the model parameters using gradient descent. Because they are trainable, they should improve during model training, hopefully in a way that allows us to cluster in high-dimensional space users and movie titles. The embeddings are learned such that the relationship between pairs of objects in the original space is preserved in the embedding space.\nThis is essential to our problem because recommending movies to specific users is like finding clusters of those users and movies. When we want to recommend movies to a new user, we just need to find the cluster to which this user belongs.\nOne hyperparameter we need to choose is the embedding dimension. The higher the dimension, the more accurate the embeddings, but the slower to train and more prone to overfitting embedding will be. 28 was selected as the embedding dimension.\nembedding_dimension = 28 One embedding will be created for the users and one for the movie titles. Then, they will be combined into a single model. Now, let’s talk about two crucial aspects of any deep learning model: metrics and loss functions.\nMetrics Metrics play a fundamental role in evaluating the performance of deep learning models. The concept of a metric is similar to the loss function, except that the metric is not used to update the model parameters during training, just for quantifying success and failure from a human perspective.\nSince we are using embeddings, one possible metric could be the following: take all your training data (user id and movie titles) and compute a score between a given pair (positive pair) and the rest of the observations (negative pair). Suppose the score of the positive pair is much higher than the score for the negative pairs. In that case, our embeddings can correctly create clusters in the high-dimensional space, which would imply more accurate recommendations.\nThis metric is implemented in TensorFlow as follows.\nmetrics = tfrs.metrics.FactorizedTopK( candidates=movies.batch(128).map(movie_model) ) Loss function Training a deep learning model could be defined as the iterative process of updating the model parameters so that the model predictions match the true labels. From a mathematical standpoint, making model predictions match the actual labels is equivalent to minimizing a loss function. Then, the model parameters are updated during training so that the predictions match the labels. Loss functions also help evaluate the performance of deep learning models.\nTFRS (TensorFlow Recommenders) provides several loss functions. Because we are solving a retrieval stage problem, the following will be used (our previously defined metrics must be passed as input)\ntask = tfrs.tasks.Retrieval( metrics=metrics ) Model implementation Let’s sum up what we have been doing so far.\nWe have converted users and movies into unique ID integer values. These ID values have created two embeddings for users and movie titles. (Embeddings could be defined as a representation of these ID values in a higher dimensional space that represents similarity) These two embeddings can be combined into a single model that outputs a score. We have defined a metric that uses this score to quantify our model’s performance. We have defined a loss function. Now, let’s put all the pieces together and implement our model (Explanations about the code are included with comments)\nclass NoBaseClassMovielensModel(tf.keras.Model): def __init__(self, user_model, movie_model): super().__init__() self.movie_model: tf.keras.Model = movie_model self.user_model: tf.keras.Model = user_model self.task: tf.keras.layers.Layer = task def train_step(self, features: Dict[Text, tf.Tensor]) -\u003e tf.Tensor: with tf.GradientTape() as tape: user_embeddings = self.user_model(features[\"user_id\"]) positive_movie_embeddings = self.movie_model(features[\"movie_title\"]) loss = self.task(user_embeddings, positive_movie_embeddings) regularization_loss = sum(self.losses) total_loss = loss + regularization_loss gradients = tape.gradient(total_loss, self.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) metrics = {metric.name: metric.result() for metric in self.metrics} metrics[\"loss\"] = loss metrics[\"regularization_loss\"] = regularization_loss metrics[\"total_loss\"] = total_loss return metrics def test_step(self, features: Dict[Text, tf.Tensor]) -\u003e tf.Tensor: user_embeddings = self.user_model(features[\"user_id\"]) positive_movie_embeddings = self.movie_model(features[\"movie_title\"]) loss = self.task(user_embeddings, positive_movie_embeddings) regularization_loss = sum(self.losses) total_loss = loss + regularization_loss metrics = {metric.name: metric.result() for metric in self.metrics} metrics[\"loss\"] = loss metrics[\"regularization_loss\"] = regularization_loss metrics[\"total_loss\"] = total_loss return metrics Model fitting and evaluation We instantiate and compile the model. The optimizer and the learning rate determine how and how much the model parameters are updated during training. A standard choice is the Adam optimizer, with a learning rate of 0.1.\nmodel = MovielensModel(user_model, movie_model) model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1)) Then shuffle, batch, and cache the training and evaluation data. Gradient descent works better when data points are independently and identically distributed (IDD), and shuffling ensures this; our neural network is not batches of data, and the cache is used to speed up training.\ncached_train = train.shuffle(100000).batch(8192).cache() cached_test = test.batch(4096).cache() We train the model during three epochs. We can add a TensorBoard callback to the fit method to visualize better model training.\nmodel.fit(cached_train, epochs=5) Epoch 1/5 10/10 [==============================] - 36s 3s/step - factorized_top_k/top_1_categorical_accuracy: 8.1250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0072 - factorized_top_k/top_10_categorical_accuracy: 0.0165 - factorized_top_k/top_50_categorical_accuracy: 0.0916 - factorized_top_k/top_100_categorical_accuracy: 0.1671 - loss: 69862.4574 - regularization_loss: 0.0000e+00 - total_loss: 69862.4574 Epoch 2/5 10/10 [==============================] - 32s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0032 - factorized_top_k/top_5_categorical_accuracy: 0.0195 - factorized_top_k/top_10_categorical_accuracy: 0.0390 - factorized_top_k/top_50_categorical_accuracy: 0.1670 - factorized_top_k/top_100_categorical_accuracy: 0.2894 - loss: 67594.8033 - regularization_loss: 0.0000e+00 - total_loss: 67594.8033 Epoch 3/5 10/10 [==============================] - 29s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0036 - factorized_top_k/top_5_categorical_accuracy: 0.0232 - factorized_top_k/top_10_categorical_accuracy: 0.0450 - factorized_top_k/top_50_categorical_accuracy: 0.1848 - factorized_top_k/top_100_categorical_accuracy: 0.3121 - loss: 66420.7017 - regularization_loss: 0.0000e+00 - total_loss: 66420.7017 Epoch 4/5 10/10 [==============================] - 29s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0038 - factorized_top_k/top_5_categorical_accuracy: 0.0252 - factorized_top_k/top_10_categorical_accuracy: 0.0491 - factorized_top_k/top_50_categorical_accuracy: 0.1980 - factorized_top_k/top_100_categorical_accuracy: 0.3289 - loss: 65761.9844 - regularization_loss: 0.0000e+00 - total_loss: 65761.9844 Epoch 5/5 10/10 [==============================] - 29s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0039 - factorized_top_k/top_5_categorical_accuracy: 0.0267 - factorized_top_k/top_10_categorical_accuracy: 0.05 Model evaluation And then finally, evaluate the model on the test set.\nmodel.evaluate(cached_test, return_dict=True) The performance on the test set is expected to be worse because its predictions are computed on data that has yet to be seen (there is always the risk of overfitting our model during training, so one must be careful).\n5/5 [==============================] - 10s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0010 - factorized_top_k/top_5_categorical_accuracy: 0.0070 - factorized_top_k/top_10_categorical_accuracy: 0.0161 - factorized_top_k/top_50_categorical_accuracy: 0.1103 - factorized_top_k/top_100_categorical_accuracy: 0.2186 - loss: 31147.1562 - regularization_loss: 0.0000e+00 - total_loss: 31147.1562 {'factorized_top_k/top_1_categorical_accuracy': 0.0010499999625608325, 'factorized_top_k/top_5_categorical_accuracy': 0.007000000216066837, 'factorized_top_k/top_10_categorical_accuracy': 0.016100000590085983, 'factorized_top_k/top_50_categorical_accuracy': 0.11029999703168869, 'factorized_top_k/top_100_categorical_accuracy': 0.2186499983072281, 'loss': 28311.09375, 'regularization_loss': 0, 'total_loss': 28311.09375} Making predictions With the following code, we can make predictions for a given user.\nindex = tfrs.layers.factorized_top_k.BruteForce(model.user_model) index.index_from_dataset( tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model))) ) # Get recommendations for user 42 _, titles = index(tf.constant([\"42\"])) %timeit _, titles = index(np.array([\"42\"]), k=3) print(f\"Recommendations for user 42: {titles[0, :3]}\") Model serving To deploy a TensorFlow model, we only need the brute force layer run previously as well as the next piece of code.\nwith tempfile.TemporaryDirectory() as tmp: path = os.path.join(tmp, \"model\") # Save the index. tf.saved_model.save(index, path) # Load it back; can also be done in TensorFlow Serving. loaded = tf.saved_model.load(path) # Pass a user id in, get top predicted movie titles back. scores, titles = loaded([\"42\"]) print(f\"Recommendations: {titles[0][:3]}\") Recommendations for user 42: [b'Rudy (1993)' b'Tombstone (1993)' b'Winnie the Pooh and the Blustery Day (1968)'] Efficient serving with ScaNN One of the drawbacks of using BruteForce is that our model will be much slower when making predictions. This could be problematic if latency is an essential requirement. We can use ScaNN to solve it.\nScaNN is a library from Google Research that indexes embeddings to allow them to be rapidly searched at inference time. It uses state-of-the-art vector compression techniques and can significantly outperform brute force search while losing minimal accuracy. To illustrate this idea, let’s create a vast number of candidates and compare the computational time between ScaNN and BruteForce.\nlots_of_movies = tf.data.Dataset.concatenate( movies.batch(8192), movies.batch(8192).repeat(1_000).map(lambda x: tf.zeros_like(x)) ) # We also add lots of dummy embeddings by randomly perturbing # the estimated embeddings for real movies. lots_of_movies_embeddings = tf.data.Dataset.concatenate( movies.batch(8192).map(model.movie_model), movies.batch(8192).repeat(1_000) .map(lambda x: model.movie_model(x)) .map(lambda x: x * tf.random.uniform(tf.shape(x))) ) BruteForce\nbrute_force_lots = tfrs.layers.factorized_top_k.BruteForce() brute_force_lots.index_from_dataset( tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings)) ) %timeit _, titles = brute_force_lots(model.user_model(np.array([\"42\"])), k=3) print(f\"Top recommendations: {titles[0]}\") 41.2 ms ± 4.29 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Top recommendations: [b'Tombstone (1993)' b\"Preacher's Wife, The (1996)\" b'American President, The (1995)' b'Dumbo (1941)' b'Rudy (1993)' b\"Kid in King Arthur's Court, A (1995)\" b'Client, The (1994)' ScaNN\nscann = tfrs.layers.factorized_top_k.ScaNN( num_reordering_candidates=500, num_leaves_to_search=30 ) scann.index_from_dataset( tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings)) ) %timeit _, titles = scann(model.user_model(np.array([\"42\"])), k=3) print(f\"Top recommendations: {titles[0]}\") 3.69 ms ± 261 µs per loop (mean ± std. dev. of 7 runs, 1 loop each) Top recommendations: [b'Tombstone (1993)' b\"Preacher's Wife, The (1996)\" b'American President, The (1995)' b'Dumbo (1941)' b'Rudy (1993)' b\"Kid in King Arthur's Court, A (1995)\" b'Client, The (1994)' After running both, we can observe that while both BruteForce and ScaNN obtain the same top 7 predictions predictions, but ScaNN is much faster. We improve inference time without an apparent loss of accuracy. Nice!\nIn addition, the advantage of ScaNN over BruteForce will grow larger as the number of candidates increases.\nConclusions ",
  "wordCount" : "2274",
  "inLanguage": "en",
  "datePublished": "2023-02-11T22:23:18+01:00",
  "dateModified": "2023-02-11T22:23:18+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://andresruizc.github.io/andresr/posts/retrieval_tf/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Andres Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://andresruizc.github.io/andresr/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://andresruizc.github.io/andresr/" accesskey="h" title="Andres Blog (Alt + H)">Andres Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://andresruizc.github.io/andresr/archive/index.html" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://andresruizc.github.io/andresr/search/index.html" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://andresruizc.github.io/andresr/quotes/index.html" title="Quotes">
                    <span>Quotes</span>
                </a>
            </li>
            <li>
                <a href="https://andresruizc.github.io/andresr/reading_list/index.html" title="Reading">
                    <span>Reading</span>
                </a>
            </li>
            <li>
                <a href="https://andresruizc.github.io/andresr/tags/index.html" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Retrieval stage recommender systems with TensorFlow
    </h1>
    <div class="post-meta"><span title='2023-02-11 22:23:18 +0100 CET'>February 11, 2023</span>&nbsp;·&nbsp;11 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction-to-recommender-systems" aria-label="Introduction to recommender systems">Introduction to recommender systems</a></li>
                <li>
                    <a href="#dataset" aria-label="Dataset">Dataset</a></li>
                <li>
                    <a href="#training-and-test-sets" aria-label="Training and test sets">Training and test sets</a></li>
                <li>
                    <a href="#embeddings" aria-label="Embeddings">Embeddings</a></li>
                <li>
                    <a href="#metrics" aria-label="Metrics">Metrics</a></li>
                <li>
                    <a href="#loss-function" aria-label="Loss function">Loss function</a></li>
                <li>
                    <a href="#model-implementation" aria-label="Model implementation">Model implementation</a></li>
                <li>
                    <a href="#model-fitting-and-evaluation" aria-label="Model fitting and evaluation">Model fitting and evaluation</a></li>
                <li>
                    <a href="#model-evaluation" aria-label="Model evaluation">Model evaluation</a></li>
                <li>
                    <a href="#making-predictions" aria-label="Making predictions">Making predictions</a></li>
                <li>
                    <a href="#model-serving" aria-label="Model serving">Model serving</a></li>
                <li>
                    <a href="#efficient-serving-with-scann" aria-label="Efficient serving with ScaNN">Efficient serving with ScaNN</a></li>
                <li>
                    <a href="#conclusions" aria-label="Conclusions">Conclusions</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="introduction-to-recommender-systems">Introduction to recommender systems<a hidden class="anchor" aria-hidden="true" href="#introduction-to-recommender-systems">#</a></h3>
<p>Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations.</p>
<p>Here are some types of recommender systems:</p>
<ol>
<li><strong>Collaborative Filtering</strong>: Assumes people with similar preferences will have similar preferences in the future. If the person one like items A and B and person 2 likes item A, person two will likely want item B.</li>
<li><strong>Content-Based Filtering</strong>: Assumes people will prefer items like those they liked in the past. Suppose I bought a pair of shoes.</li>
<li><strong>Hybrid Recommender Systems</strong>: Combines both collaborative and content-based filtering.</li>
<li><strong>Matrix Factorization</strong>: Assumes hidden relationships in the data containing both users and items may exist.</li>
<li><strong>Association Rule Mining</strong>: Assumes items frequently purchased together are likely to interest the user.</li>
</ol>
<p>It is very common to work with vast amounts of data when building recommender systems, and computational and time requirements must be considered. That’s why real-world recommender systems are often divided into two stages:</p>
<ol>
<li><strong>The retrieval stage</strong> filters an initial set of potential candidates out of all possible ones. It must be computationally efficient because candidates are in the order of millions.</li>
<li><strong>The ranking stage</strong> uses the outputs of the retrieval model and selects the most likely candidates.</li>
</ol>
<p>In this tutorial, I will focus on the retrieval stage. Specifically, I will build a retrieval stage recommender system that predicts a set of movies a given user is likely to watch using TensorFlow and the MovieLens dataset.</p>
<h3 id="dataset">Dataset<a hidden class="anchor" aria-hidden="true" href="#dataset">#</a></h3>
<p>The MovieLens dataset contains 100,000 ratings and 3,600 tags applied to 9,000 movies by 600 users, and it can be downloaded directly from TensorFlow Datasets. Both the datasets containing the movies and the rating will be downloaded, but we will only focus on the movies dataset.</p>
<p>Let’s load both datasets and visualize the attributes of each observation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#75715e"># Package installation</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>pip install <span style="color:#f92672">-</span>q tensorflow<span style="color:#f92672">-</span>recommenders
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>pip install <span style="color:#f92672">-</span>q <span style="color:#f92672">--</span>upgrade tensorflow<span style="color:#f92672">-</span>datasets
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>pip install <span style="color:#f92672">-</span>q scann
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Loading libraries</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pprint
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tempfile
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Dict, Text
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow_datasets <span style="color:#66d9ef">as</span> tfds
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow_recommenders <span style="color:#66d9ef">as</span> tfrs
</span></span></code></pre></div><p>Let&rsquo;s load both datasets</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#75715e"># Ratings data.</span>
</span></span><span style="display:flex;"><span>ratings <span style="color:#f92672">=</span> tfds<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;movielens/100k-ratings&#34;</span>, split<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;train&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Features of all the available movies.</span>
</span></span><span style="display:flex;"><span>movies <span style="color:#f92672">=</span> tfds<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;movielens/100k-movies&#34;</span>, split<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;train&#34;</span>)
</span></span></code></pre></div><p>And visualize their attributes</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> ratings<span style="color:#f92672">.</span>take(<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>as_numpy_iterator():
</span></span><span style="display:flex;"><span>  pprint<span style="color:#f92672">.</span>pprint(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> movies<span style="color:#f92672">.</span>take(<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>as_numpy_iterator():
</span></span><span style="display:flex;"><span>  pprint<span style="color:#f92672">.</span>pprint(x)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>Rating dataset:
</span></span><span style="display:flex;"><span>{&#39;bucketized_user_age&#39;: 45.0,
</span></span><span style="display:flex;"><span>	&#39;movie_genres&#39;: array([7]),
</span></span><span style="display:flex;"><span>	&#39;movie_id&#39;: b&#39;357&#39;,
</span></span><span style="display:flex;"><span> 	&#39;movie_title&#39;: b&#34;One Flew Over the Cuckoo&#39;s Nest (1975)&#34;,
</span></span><span style="display:flex;"><span> 	&#39;raw_user_age&#39;: 46.0,
</span></span><span style="display:flex;"><span> 	&#39;timestamp&#39;: 879024327,
</span></span><span style="display:flex;"><span> 	&#39;user_gender&#39;: True,
</span></span><span style="display:flex;"><span> 	&#39;user_id&#39;: b&#39;138&#39;,
</span></span><span style="display:flex;"><span> 	&#39;user_occupation_label&#39;: 4,
</span></span><span style="display:flex;"><span> 	&#39;user_occupation_text&#39;: b&#39;doctor&#39;,
</span></span><span style="display:flex;"><span> 	&#39;user_rating&#39;: 4.0,
</span></span><span style="display:flex;"><span> 	&#39;user_zip_code&#39;: b&#39;53211&#39;}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Movie dataset:
</span></span><span style="display:flex;"><span>{&#39;movie_genres&#39;: array([4]),
</span></span><span style="display:flex;"><span> &#39;movie_id&#39;: b&#39;1681&#39;,
</span></span><span style="display:flex;"><span> &#39;movie_title&#39;: b&#39;You So Crazy (1994)&#39;}
</span></span></code></pre></div><p>We only need the user_id and movie_title fields to build our retrieval stage. We are using TensorFlow for the data preprocessing stages because TensorFlow uses a specific data storage and message format that is targeted for ML workloads.Still, we could use other frameworks, such as Pandas, and then convert them back to a suitable form.</p>
<p>data storage and message format that is targeted for ML workloads.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span>ratings <span style="color:#f92672">=</span> ratings<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;movie_title&#34;</span>: x[<span style="color:#e6db74">&#34;movie_title&#34;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;user_id&#34;</span>: x[<span style="color:#e6db74">&#34;user_id&#34;</span>],
</span></span><span style="display:flex;"><span>})
</span></span><span style="display:flex;"><span>movies <span style="color:#f92672">=</span> movies<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x[<span style="color:#e6db74">&#34;movie_title&#34;</span>])
</span></span></code></pre></div><h3 id="training-and-test-sets">Training and test sets<a hidden class="anchor" aria-hidden="true" href="#training-and-test-sets">#</a></h3>
<p>Why do we need to split our data into training and test sets? Because one of the most critical aspects of ML models is their ability to generalize well on unseen instances. If we were to use all our data to train the model and then evaluate model performance, we would not be capable of assessing how good our model is. Therefore, we need a training and test set.</p>
<p>A good rule of thumb is to divide 80% for training and 20% for testing, and we’ll stick to that, but it depends on the specifics of your problem.</p>
<p>The strategy we use to split our data is also essential. If we are working with time series, we would have to divide based on timestamps. And the same applies to industrial recommender systems. In this tutorial, we won’t use any of that because our problem is simple.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>set_seed(<span style="color:#ae81ff">42</span>) <span style="color:#75715e"># to get reproducible partitions</span>
</span></span><span style="display:flex;"><span>shuffled <span style="color:#f92672">=</span> ratings<span style="color:#f92672">.</span>shuffle(<span style="color:#ae81ff">100000</span>, seed<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>, reshuffle_each_iteration<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train <span style="color:#f92672">=</span> shuffled<span style="color:#f92672">.</span>take(<span style="color:#ae81ff">80000</span>) <span style="color:#75715e"># 80%</span>
</span></span><span style="display:flex;"><span>test <span style="color:#f92672">=</span> shuffled<span style="color:#f92672">.</span>skip(<span style="color:#ae81ff">80000</span>)<span style="color:#f92672">.</span>take(<span style="color:#ae81ff">20000</span>) <span style="color:#75715e"># 20%</span>
</span></span></code></pre></div><p>We also need to create lookup tables with unique values for our users and movie titles so that we can later use embeddings.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>movie_titles <span style="color:#f92672">=</span> movies<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>user_ids <span style="color:#f92672">=</span> ratings<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">1000000</span>)<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x[<span style="color:#e6db74">&#34;user_id&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>unique_movie_titles <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>unique(np<span style="color:#f92672">.</span>concatenate(list(movie_titles)))
</span></span><span style="display:flex;"><span>unique_user_ids <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>unique(np<span style="color:#f92672">.</span>concatenate(list(user_ids)))
</span></span></code></pre></div><h3 id="embeddings">Embeddings<a hidden class="anchor" aria-hidden="true" href="#embeddings">#</a></h3>
<p>Now that we have a lookup table with unique values for our users and movie titles, we can use embeddings.
Embeddings are dense representations of high-dimensional data, such as words or vocabulary. They can be understood as a lookup table that maps integer indices (e.g., user id and movie titles) to dense vectors.</p>
<p>Embeddings usually are initialized randomly, then trained using backpropagation along with the rest of the model parameters using gradient descent. Because they are trainable, they should improve during model training, hopefully in a way that allows us to cluster in high-dimensional space users and movie titles. The embeddings are learned such that the relationship between pairs of objects in the original space is preserved in the embedding space.</p>
<p>This is essential to our problem because recommending movies to specific users is like finding clusters of those users and movies. When we want to recommend movies to a new user, we just need to find the cluster to which this user belongs.</p>
<p>One hyperparameter we need to choose is the embedding dimension. The higher the dimension, the more accurate the embeddings, but the slower to train and more prone to overfitting embedding will be. 28 was selected as the embedding dimension.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>embedding_dimension <span style="color:#f92672">=</span> <span style="color:#ae81ff">28</span>
</span></span></code></pre></div><p>One embedding will be created for the users and one for the movie titles. Then, they will be combined into a single model. Now, let’s talk about two crucial aspects of any deep learning model: metrics and loss functions.</p>
<h3 id="metrics">Metrics<a hidden class="anchor" aria-hidden="true" href="#metrics">#</a></h3>
<p>Metrics play a fundamental role in evaluating the performance of deep learning models. The concept of a metric is similar to the loss function, except that the metric is not used to update the model parameters during training, just for quantifying success and failure from a human perspective.</p>
<p>Since we are using embeddings, one possible metric could be the following: take all your training data (user id and movie titles) and compute a score between a given pair (positive pair) and the rest of the observations (negative pair). Suppose the score of the positive pair is much higher than the score for the negative pairs. In that case, our embeddings can correctly create clusters in the high-dimensional space, which would imply more accurate recommendations.</p>
<p>This metric is implemented in TensorFlow as follows.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>metrics <span style="color:#f92672">=</span> tfrs<span style="color:#f92672">.</span>metrics<span style="color:#f92672">.</span>FactorizedTopK(
</span></span><span style="display:flex;"><span>  candidates<span style="color:#f92672">=</span>movies<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">128</span>)<span style="color:#f92672">.</span>map(movie_model)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="loss-function">Loss function<a hidden class="anchor" aria-hidden="true" href="#loss-function">#</a></h3>
<p>Training a deep learning model could be defined as the iterative process of updating the model parameters so that the model predictions match the true labels. From a mathematical standpoint, making model predictions match the actual labels is equivalent to minimizing a loss function. Then, the model parameters are updated during training so that the predictions match the labels. Loss functions also help evaluate the performance of deep learning models.</p>
<p>TFRS (TensorFlow Recommenders) provides several loss functions. Because we are solving a retrieval stage problem, the following will be used (our previously defined metrics must be passed as input)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>task <span style="color:#f92672">=</span> tfrs<span style="color:#f92672">.</span>tasks<span style="color:#f92672">.</span>Retrieval(
</span></span><span style="display:flex;"><span>  metrics<span style="color:#f92672">=</span>metrics
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="model-implementation">Model implementation<a hidden class="anchor" aria-hidden="true" href="#model-implementation">#</a></h3>
<p>Let’s sum up what we have been doing so far.</p>
<ol>
<li>We have converted users and movies into unique ID integer values.</li>
<li>These ID values have created two embeddings for users and movie titles. (Embeddings could be defined as a representation of these ID values in a higher dimensional space that represents similarity)</li>
<li>These two embeddings can be combined into a single model that outputs a score.</li>
<li>We have defined a metric that uses this score to quantify our model&rsquo;s performance.</li>
<li>We have defined a loss function.</li>
</ol>
<p>Now, let’s put all the pieces together and implement our model (Explanations about the code are included with comments)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">NoBaseClassMovielensModel</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> __init__(self, user_model, movie_model):
</span></span><span style="display:flex;"><span>    super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>movie_model: tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model <span style="color:#f92672">=</span> movie_model
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>user_model: tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model <span style="color:#f92672">=</span> user_model
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>task: tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Layer <span style="color:#f92672">=</span> task
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_step</span>(self, features: Dict[Text, tf<span style="color:#f92672">.</span>Tensor]) <span style="color:#f92672">-&gt;</span> tf<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      user_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>user_model(features[<span style="color:#e6db74">&#34;user_id&#34;</span>])
</span></span><span style="display:flex;"><span>      positive_movie_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>movie_model(features[<span style="color:#e6db74">&#34;movie_title&#34;</span>])
</span></span><span style="display:flex;"><span>      loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>task(user_embeddings, positive_movie_embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      regularization_loss <span style="color:#f92672">=</span> sum(self<span style="color:#f92672">.</span>losses)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      total_loss <span style="color:#f92672">=</span> loss <span style="color:#f92672">+</span> regularization_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(total_loss, self<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>optimizer<span style="color:#f92672">.</span>apply_gradients(zip(gradients, self<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    metrics <span style="color:#f92672">=</span> {metric<span style="color:#f92672">.</span>name: metric<span style="color:#f92672">.</span>result() <span style="color:#66d9ef">for</span> metric <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>metrics}
</span></span><span style="display:flex;"><span>    metrics[<span style="color:#e6db74">&#34;loss&#34;</span>] <span style="color:#f92672">=</span> loss
</span></span><span style="display:flex;"><span>    metrics[<span style="color:#e6db74">&#34;regularization_loss&#34;</span>] <span style="color:#f92672">=</span> regularization_loss
</span></span><span style="display:flex;"><span>    metrics[<span style="color:#e6db74">&#34;total_loss&#34;</span>] <span style="color:#f92672">=</span> total_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> metrics
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_step</span>(self, features: Dict[Text, tf<span style="color:#f92672">.</span>Tensor]) <span style="color:#f92672">-&gt;</span> tf<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    user_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>user_model(features[<span style="color:#e6db74">&#34;user_id&#34;</span>])
</span></span><span style="display:flex;"><span>    positive_movie_embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>movie_model(features[<span style="color:#e6db74">&#34;movie_title&#34;</span>])
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>task(user_embeddings, positive_movie_embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    regularization_loss <span style="color:#f92672">=</span> sum(self<span style="color:#f92672">.</span>losses)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    total_loss <span style="color:#f92672">=</span> loss <span style="color:#f92672">+</span> regularization_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    metrics <span style="color:#f92672">=</span> {metric<span style="color:#f92672">.</span>name: metric<span style="color:#f92672">.</span>result() <span style="color:#66d9ef">for</span> metric <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>metrics}
</span></span><span style="display:flex;"><span>    metrics[<span style="color:#e6db74">&#34;loss&#34;</span>] <span style="color:#f92672">=</span> loss
</span></span><span style="display:flex;"><span>    metrics[<span style="color:#e6db74">&#34;regularization_loss&#34;</span>] <span style="color:#f92672">=</span> regularization_loss
</span></span><span style="display:flex;"><span>    metrics[<span style="color:#e6db74">&#34;total_loss&#34;</span>] <span style="color:#f92672">=</span> total_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> metrics
</span></span></code></pre></div><h3 id="model-fitting-and-evaluation">Model fitting and evaluation<a hidden class="anchor" aria-hidden="true" href="#model-fitting-and-evaluation">#</a></h3>
<p>We instantiate and compile the model. The optimizer and the learning rate determine how and how much the model parameters are updated during training. A standard choice is the Adam optimizer, with a learning rate of 0.1.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> MovielensModel(user_model, movie_model)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adagrad(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>))
</span></span></code></pre></div><p>Then shuffle, batch, and cache the training and evaluation data. Gradient descent works better when data points are independently and identically distributed (IDD), and shuffling ensures this; our neural network is not batches of data, and the cache is used to speed up training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cached_train <span style="color:#f92672">=</span> train<span style="color:#f92672">.</span>shuffle(<span style="color:#ae81ff">100000</span>)<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">8192</span>)<span style="color:#f92672">.</span>cache()
</span></span><span style="display:flex;"><span>cached_test <span style="color:#f92672">=</span> test<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">4096</span>)<span style="color:#f92672">.</span>cache()
</span></span></code></pre></div><p>We train the model during three epochs. We can add a TensorBoard callback to the fit method to visualize better model training.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(cached_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>Epoch 1/5
</span></span><span style="display:flex;"><span>10/10 [==============================] - 36s 3s/step - factorized_top_k/top_1_categorical_accuracy: 8.1250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0072 - factorized_top_k/top_10_categorical_accuracy: 0.0165 - factorized_top_k/top_50_categorical_accuracy: 0.0916 - factorized_top_k/top_100_categorical_accuracy: 0.1671 - loss: 69862.4574 - regularization_loss: 0.0000e+00 - total_loss: 69862.4574
</span></span><span style="display:flex;"><span>Epoch 2/5
</span></span><span style="display:flex;"><span>10/10 [==============================] - 32s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0032 - factorized_top_k/top_5_categorical_accuracy: 0.0195 - factorized_top_k/top_10_categorical_accuracy: 0.0390 - factorized_top_k/top_50_categorical_accuracy: 0.1670 - factorized_top_k/top_100_categorical_accuracy: 0.2894 - loss: 67594.8033 - regularization_loss: 0.0000e+00 - total_loss: 67594.8033
</span></span><span style="display:flex;"><span>Epoch 3/5
</span></span><span style="display:flex;"><span>10/10 [==============================] - 29s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0036 - factorized_top_k/top_5_categorical_accuracy: 0.0232 - factorized_top_k/top_10_categorical_accuracy: 0.0450 - factorized_top_k/top_50_categorical_accuracy: 0.1848 - factorized_top_k/top_100_categorical_accuracy: 0.3121 - loss: 66420.7017 - regularization_loss: 0.0000e+00 - total_loss: 66420.7017
</span></span><span style="display:flex;"><span>Epoch 4/5
</span></span><span style="display:flex;"><span>10/10 [==============================] - 29s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0038 - factorized_top_k/top_5_categorical_accuracy: 0.0252 - factorized_top_k/top_10_categorical_accuracy: 0.0491 - factorized_top_k/top_50_categorical_accuracy: 0.1980 - factorized_top_k/top_100_categorical_accuracy: 0.3289 - loss: 65761.9844 - regularization_loss: 0.0000e+00 - total_loss: 65761.9844
</span></span><span style="display:flex;"><span>Epoch 5/5
</span></span><span style="display:flex;"><span>10/10 [==============================] - 29s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0039 - factorized_top_k/top_5_categorical_accuracy: 0.0267 - factorized_top_k/top_10_categorical_accuracy: 0.05
</span></span></code></pre></div><h3 id="model-evaluation">Model evaluation<a hidden class="anchor" aria-hidden="true" href="#model-evaluation">#</a></h3>
<p>And then finally, evaluate the model on the test set.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model<span style="color:#f92672">.</span>evaluate(cached_test, return_dict<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p>The performance on the test set is expected to be worse because its predictions are computed on data that has yet to be seen (there is always the risk of overfitting our model during training, so one must be careful).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>5/5 [==============================] - 10s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0010 - factorized_top_k/top_5_categorical_accuracy: 0.0070 - factorized_top_k/top_10_categorical_accuracy: 0.0161 - factorized_top_k/top_50_categorical_accuracy: 0.1103 - factorized_top_k/top_100_categorical_accuracy: 0.2186 - loss: 31147.1562 - regularization_loss: 0.0000e+00 - total_loss: 31147.1562
</span></span><span style="display:flex;"><span>{&#39;factorized_top_k/top_1_categorical_accuracy&#39;: 0.0010499999625608325,
</span></span><span style="display:flex;"><span> &#39;factorized_top_k/top_5_categorical_accuracy&#39;: 0.007000000216066837,
</span></span><span style="display:flex;"><span> &#39;factorized_top_k/top_10_categorical_accuracy&#39;: 0.016100000590085983,
</span></span><span style="display:flex;"><span> &#39;factorized_top_k/top_50_categorical_accuracy&#39;: 0.11029999703168869,
</span></span><span style="display:flex;"><span> &#39;factorized_top_k/top_100_categorical_accuracy&#39;: 0.2186499983072281,
</span></span><span style="display:flex;"><span> &#39;loss&#39;: 28311.09375,
</span></span><span style="display:flex;"><span> &#39;regularization_loss&#39;: 0,
</span></span><span style="display:flex;"><span> &#39;total_loss&#39;: 28311.09375}
</span></span></code></pre></div><h3 id="making-predictions">Making predictions<a hidden class="anchor" aria-hidden="true" href="#making-predictions">#</a></h3>
<p>With the following code, we can make predictions for a given user.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>index <span style="color:#f92672">=</span> tfrs<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>factorized_top_k<span style="color:#f92672">.</span>BruteForce(model<span style="color:#f92672">.</span>user_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>index<span style="color:#f92672">.</span>index_from_dataset(
</span></span><span style="display:flex;"><span>  tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>zip((movies<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">100</span>), movies<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">100</span>)<span style="color:#f92672">.</span>map(model<span style="color:#f92672">.</span>movie_model)))
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Get recommendations for user 42</span>
</span></span><span style="display:flex;"><span>_, titles <span style="color:#f92672">=</span> index(tf<span style="color:#f92672">.</span>constant([<span style="color:#e6db74">&#34;42&#34;</span>]))
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>timeit _, titles <span style="color:#f92672">=</span> index(np<span style="color:#f92672">.</span>array([<span style="color:#e6db74">&#34;42&#34;</span>]), k<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Recommendations for user 42: </span><span style="color:#e6db74">{</span>titles[<span style="color:#ae81ff">0</span>, :<span style="color:#ae81ff">3</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h3 id="model-serving">Model serving<a hidden class="anchor" aria-hidden="true" href="#model-serving">#</a></h3>
<p>To deploy a TensorFlow model, we only need the brute force layer run previously as well as the next piece of code.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">with</span> tempfile<span style="color:#f92672">.</span>TemporaryDirectory() <span style="color:#66d9ef">as</span> tmp:
</span></span><span style="display:flex;"><span>  path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(tmp, <span style="color:#e6db74">&#34;model&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Save the index.</span>
</span></span><span style="display:flex;"><span>  tf<span style="color:#f92672">.</span>saved_model<span style="color:#f92672">.</span>save(index, path)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Load it back; can also be done in TensorFlow Serving.</span>
</span></span><span style="display:flex;"><span>  loaded <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>saved_model<span style="color:#f92672">.</span>load(path)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Pass a user id in, get top predicted movie titles back.</span>
</span></span><span style="display:flex;"><span>  scores, titles <span style="color:#f92672">=</span> loaded([<span style="color:#e6db74">&#34;42&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Recommendations: </span><span style="color:#e6db74">{</span>titles[<span style="color:#ae81ff">0</span>][:<span style="color:#ae81ff">3</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>Recommendations for user 42: [b&#39;Rudy (1993)&#39; b&#39;Tombstone (1993)&#39;
</span></span><span style="display:flex;"><span> b&#39;Winnie the Pooh and the Blustery Day (1968)&#39;]
</span></span></code></pre></div><h3 id="efficient-serving-with-scann">Efficient serving with ScaNN<a hidden class="anchor" aria-hidden="true" href="#efficient-serving-with-scann">#</a></h3>
<p>One of the drawbacks of using BruteForce is that our model will be much slower when making predictions. This could be problematic if latency is an essential requirement. We can use ScaNN to solve it.</p>
<p>ScaNN is a library from Google Research that indexes embeddings to allow them to be rapidly searched at inference time. It uses state-of-the-art vector compression techniques and can significantly outperform brute force search while losing minimal accuracy. To illustrate this idea, let’s create a vast number of candidates and compare the computational time between ScaNN and BruteForce.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lots_of_movies <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>concatenate(
</span></span><span style="display:flex;"><span>    movies<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">8192</span>),
</span></span><span style="display:flex;"><span>    movies<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">8192</span>)<span style="color:#f92672">.</span>repeat(<span style="color:#ae81ff">1_000</span>)<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: tf<span style="color:#f92672">.</span>zeros_like(x))
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We also add lots of dummy embeddings by randomly perturbing</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># the estimated embeddings for real movies.</span>
</span></span><span style="display:flex;"><span>lots_of_movies_embeddings <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>concatenate(
</span></span><span style="display:flex;"><span>    movies<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">8192</span>)<span style="color:#f92672">.</span>map(model<span style="color:#f92672">.</span>movie_model),
</span></span><span style="display:flex;"><span>    movies<span style="color:#f92672">.</span>batch(<span style="color:#ae81ff">8192</span>)<span style="color:#f92672">.</span>repeat(<span style="color:#ae81ff">1_000</span>)
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: model<span style="color:#f92672">.</span>movie_model(x))
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(tf<span style="color:#f92672">.</span>shape(x)))
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>BruteForce</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>brute_force_lots <span style="color:#f92672">=</span> tfrs<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>factorized_top_k<span style="color:#f92672">.</span>BruteForce()
</span></span><span style="display:flex;"><span>brute_force_lots<span style="color:#f92672">.</span>index_from_dataset(
</span></span><span style="display:flex;"><span>    tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>zip((lots_of_movies, lots_of_movies_embeddings))
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>timeit _, titles <span style="color:#f92672">=</span> brute_force_lots(model<span style="color:#f92672">.</span>user_model(np<span style="color:#f92672">.</span>array([<span style="color:#e6db74">&#34;42&#34;</span>])), k<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Top recommendations: </span><span style="color:#e6db74">{</span>titles[<span style="color:#ae81ff">0</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>41.2 ms ± 4.29 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Top recommendations: [b&#39;Tombstone (1993)&#39; b&#34;Preacher&#39;s Wife, The (1996)&#34;
</span></span><span style="display:flex;"><span> b&#39;American President, The (1995)&#39; b&#39;Dumbo (1941)&#39; b&#39;Rudy (1993)&#39;
</span></span><span style="display:flex;"><span> b&#34;Kid in King Arthur&#39;s Court, A (1995)&#34; b&#39;Client, The (1994)&#39;
</span></span></code></pre></div><p>ScaNN</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>scann <span style="color:#f92672">=</span> tfrs<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>factorized_top_k<span style="color:#f92672">.</span>ScaNN(
</span></span><span style="display:flex;"><span>    num_reordering_candidates<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>,
</span></span><span style="display:flex;"><span>    num_leaves_to_search<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>scann<span style="color:#f92672">.</span>index_from_dataset(
</span></span><span style="display:flex;"><span>    tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>zip((lots_of_movies, lots_of_movies_embeddings))
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>timeit _, titles <span style="color:#f92672">=</span> scann(model<span style="color:#f92672">.</span>user_model(np<span style="color:#f92672">.</span>array([<span style="color:#e6db74">&#34;42&#34;</span>])), k<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Top recommendations: </span><span style="color:#e6db74">{</span>titles[<span style="color:#ae81ff">0</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>3.69 ms ± 261 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Top recommendations: [b&#39;Tombstone (1993)&#39; b&#34;Preacher&#39;s Wife, The (1996)&#34;
</span></span><span style="display:flex;"><span> b&#39;American President, The (1995)&#39; b&#39;Dumbo (1941)&#39; b&#39;Rudy (1993)&#39;
</span></span><span style="display:flex;"><span> b&#34;Kid in King Arthur&#39;s Court, A (1995)&#34; b&#39;Client, The (1994)&#39;
</span></span></code></pre></div><p>After running both, we can observe that while both BruteForce and ScaNN obtain the same top 7 predictions predictions, but ScaNN is much faster. We improve inference time without an apparent loss of accuracy. Nice!</p>
<p>In addition, the advantage of ScaNN over BruteForce will grow larger as the number of candidates increases.</p>
<h3 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h3>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://andresruizc.github.io/andresr/tags/ml/">ML</a></li>
      <li><a href="https://andresruizc.github.io/andresr/tags/tensorflow/">TensorFlow</a></li>
      <li><a href="https://andresruizc.github.io/andresr/tags/recommender/">Recommender</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://andresruizc.github.io/andresr/">Andres Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
