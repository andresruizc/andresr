[{"content":"Introduction to recommender systems Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations.\nHere are some types of recommender systems:\nCollaborative Filtering: Assumes that people with similar preferences will have similar preferences in the future. So, if the person one like items A and B and person 2 likes item A, person two will likely want item B. Content-Based Filtering: Assumes that people will prefer items like those they liked in the past. Suppose I bought a pair of shoes. Hybrid Recommender Systems: Combines both collaborative and content-based filtering. Matrix Factorization: Assumes hidden relationships in the data containing both users and items may exist. Association Rule Mining: Assumes items frequently purchased together are likely to interest the user. It is very common to work with vast amounts of data when building recommender systems, and computational and time requirements must be considered. That’s why real-world recommender systems are often divided into two stages:\nThe retrieval stage filters an initial set of potential candidates out of all possible ones. It must be computationally efficient because candidates are in the order of millions. The ranking stage uses the outputs of the retrieval model and selects the most likely candidates. In this tutorial, I will focus on the retrieval stage. Specifically, I will build a retrieval stage recommender system that predicts a set of movies a given user is likely to watch using TensorFlow and the MovieLens dataset.\nDataset The MovieLens dataset contains 100,000 ratings and 3,600 tags applied to 9,000 movies by 600 users, and it can be downloaded directly from TensorFlow Datasets. Both the datasets containing the movies and the rating will be downloaded, but we will only focus on the movies dataset.\nLet’s load both datasets and visualize the attributes of each observation.\n# Package installation !pip install -q tensorflow-recommenders !pip install -q --upgrade tensorflow-datasets !pip install -q scann # Loading libraries import os import pprint import tempfile from typing import Dict, Text import numpy as np import tensorflow as tf import tensorflow_datasets as tfds import tensorflow_recommenders as tfrs Let\u0026rsquo;s load both datasets\n# Ratings data. ratings = tfds.load(\u0026#34;movielens/100k-ratings\u0026#34;, split=\u0026#34;train\u0026#34;) # Features of all the available movies. movies = tfds.load(\u0026#34;movielens/100k-movies\u0026#34;, split=\u0026#34;train\u0026#34;) And visualize their attributes\nfor x in ratings.take(1).as_numpy_iterator(): pprint.pprint(x) for x in movies.take(1).as_numpy_iterator(): pprint.pprint(x) Rating dataset: {\u0026#39;bucketized_user_age\u0026#39;: 45.0, \u0026#39;movie_genres\u0026#39;: array([7]), \u0026#39;movie_id\u0026#39;: b\u0026#39;357\u0026#39;, \u0026#39;movie_title\u0026#39;: b\u0026#34;One Flew Over the Cuckoo\u0026#39;s Nest (1975)\u0026#34;, \u0026#39;raw_user_age\u0026#39;: 46.0, \u0026#39;timestamp\u0026#39;: 879024327, \u0026#39;user_gender\u0026#39;: True, \u0026#39;user_id\u0026#39;: b\u0026#39;138\u0026#39;, \u0026#39;user_occupation_label\u0026#39;: 4, \u0026#39;user_occupation_text\u0026#39;: b\u0026#39;doctor\u0026#39;, \u0026#39;user_rating\u0026#39;: 4.0, \u0026#39;user_zip_code\u0026#39;: b\u0026#39;53211\u0026#39;} Movie dataset: {\u0026#39;movie_genres\u0026#39;: array([4]), \u0026#39;movie_id\u0026#39;: b\u0026#39;1681\u0026#39;, \u0026#39;movie_title\u0026#39;: b\u0026#39;You So Crazy (1994)\u0026#39;} We only need the user_id and movie_title fields to build our retrieval stage. We are using TensorFlow for the data preprocessing stages because a specific data format to build TF models is required. Still, we could use other frameworks, such as Pandas, and then convert them back to a suitable form.\nratings = ratings.map(lambda x: { \u0026#34;movie_title\u0026#34;: x[\u0026#34;movie_title\u0026#34;], \u0026#34;user_id\u0026#34;: x[\u0026#34;user_id\u0026#34;], }) movies = movies.map(lambda x: x[\u0026#34;movie_title\u0026#34;]) Training and test sets Why do we need to split our data into training and test sets? Because one of the most critical aspects of ML models is their ability to generalize well on unseen instances. If we were to use all our data to train the model and then evaluate model performance, we would not be capable of assessing how good our model is. Therefore, we need a training and test set.\nA good rule of thumb is to divide 80% for training and 20% for testing, and we’ll stick to that, but it depends on the specifics of your problem.\nThe strategy we use to split our data is also essential. If we are working with time series, we would have to divide based on timestamps. And the same applies to industrial recommender systems. In this tutorial, we won’t use any of that because our problem is simple.\ntf.random.set_seed(42) # to get reproducible partitions shuffled = ratings.shuffle(100000, seed=42, reshuffle_each_iteration=False) train = shuffled.take(80000) # 80% test = shuffled.skip(80000).take(20000) # 20% We also need to create lookup tables with unique values for our users and movie titles so that we can later use embeddings.\nmovie_titles = movies.batch(1000) user_ids = ratings.batch(1000000).map(lambda x: x[\u0026#34;user_id\u0026#34;]) unique_movie_titles = np.unique(np.concatenate(list(movie_titles))) unique_user_ids = np.unique(np.concatenate(list(user_ids))) Embeddings Now that we have a lookup table with unique values for our users and movie titles, we can use embeddings. Embeddings are dense representations of high-dimensional data, such as words or vocabulary. They can be understood as a lookup table that maps integer indices (e.g., user id and movie titles) to dense vectors.\nEmbeddings usually are initialized randomly, then trained using backpropagation along with the rest of the model parameters using gradient descent. Because they are trainable, they should improve during model training, hopefully in a way that allows us to cluster in high-dimensional space users and movie titles. The embeddings are learned such that the relationship between pairs of objects in the original space is preserved in the embedding space.\nThis is essential to our problem because recommending movies to specific users is like finding clusters of those users and movies. When we want to recommend movies to a new user, we just need to find the cluster to which this user belongs.\nOne hyperparameter we need to choose is the embedding dimension. The higher the dimension, the more accurate the embeddings, but the slower to train and more prone to overfitting embedding will be. 32 was selected as the embedding dimension.\nOne embedding will be created for the users and one for the movie titles. Then, they will be combined into a single model. Now, let’s talk about two crucial aspects of any deep learning model: loss functions and metrics.\nMetrics Metrics play a fundamental role in evaluating the performance of deep learning models. The concept of a metric is similar to the loss function, except that the metric is not used to update the model parameters during training, just for quantifying success and failure from a human perspective.\nSince we are using embeddings, one possible metric could be the following: take all your training data (user id and movie titles) and compute a score between a given pair (positive pair) and the rest of the observations (negative pair). Suppose the score of the positive pair is much higher than the score for the negative pairs. In that case, our embeddings can correctly create clusters in the high-dimensional space, which would imply more accurate recommendations.\nThis metric is implemented in TensorFlow as follows.\nmetrics = tfrs.metrics.FactorizedTopK( candidates=movies.batch(128).map(movie_model) ) Loss function Training a deep learning model could be defined as the iterative process of updating the model parameters so that the model predictions match the true labels. From a mathematical standpoint, making model predictions match the actual labels is equivalent to minimizing a loss function. Then, the model parameters are updated during training so that the predictions match the labels. Loss functions also help evaluate the performance of deep learning models.\nTFRS (TensorFlow Recommenders) provides several loss functions. Because we are solving a retrieval stage problem, the following will be used (our previously defined metrics must be passed as input)\ntask = tfrs.tasks.Retrieval( metrics=metrics ) Model implementation Let’s sum up what we have been doing so far.\nWe have converted users and movies into unique ID integer values. These ID values have created two embeddings for users and movie titles. (Embeddings could be defined as a representation of these ID values in a higher dimensional space that represents similarity) These two embeddings can be combined into a single model that outputs a score. We have defined a metric that uses this score to quantify our model\u0026rsquo;s performance. We have defined a loss function. Now, let’s put all the pieces together and implement our model (Explanations about the code are included with comments)\nclass NoBaseClassMovielensModel(tf.keras.Model): def __init__(self, user_model, movie_model): super().__init__() self.movie_model: tf.keras.Model = movie_model self.user_model: tf.keras.Model = user_model self.task: tf.keras.layers.Layer = task def train_step(self, features: Dict[Text, tf.Tensor]) -\u0026gt; tf.Tensor: # Set up a gradient tape to record gradients. with tf.GradientTape() as tape: # Loss computation. user_embeddings = self.user_model(features[\u0026#34;user_id\u0026#34;]) positive_movie_embeddings = self.movie_model(features[\u0026#34;movie_title\u0026#34;]) loss = self.task(user_embeddings, positive_movie_embeddings) # Handle regularization losses as well. regularization_loss = sum(self.losses) total_loss = loss + regularization_loss gradients = tape.gradient(total_loss, self.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) metrics = {metric.name: metric.result() for metric in self.metrics} metrics[\u0026#34;loss\u0026#34;] = loss metrics[\u0026#34;regularization_loss\u0026#34;] = regularization_loss metrics[\u0026#34;total_loss\u0026#34;] = total_loss return metrics def test_step(self, features: Dict[Text, tf.Tensor]) -\u0026gt; tf.Tensor: # Loss computation. user_embeddings = self.user_model(features[\u0026#34;user_id\u0026#34;]) positive_movie_embeddings = self.movie_model(features[\u0026#34;movie_title\u0026#34;]) loss = self.task(user_embeddings, positive_movie_embeddings) # Handle regularization losses as well. regularization_loss = sum(self.losses) total_loss = loss + regularization_loss metrics = {metric.name: metric.result() for metric in self.metrics} metrics[\u0026#34;loss\u0026#34;] = loss metrics[\u0026#34;regularization_loss\u0026#34;] = regularization_loss metrics[\u0026#34;total_loss\u0026#34;] = total_loss return metrics Model fitting and evaluation We instantiate and compile the model. The optimizer and the learning rate determine how and how much the model parameters are updated during training. A standard choice is the Adam optimizer, with a learning rate of 0.1.\nmodel = MovielensModel(user_model, movie_model) model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1)) Then shuffle, batch, and cache the training and evaluation data. Gradient descent works better when data points are independently and identically distributed (IDD), and shuffling ensures this; our neural network is not batches of data, and the cache is used to speed up training.\ncached_train = train.shuffle(100000).batch(8192).cache() cached_test = test.batch(4096).cache() We train the model during three epochs. We can add a TensorBoard callback to the fit method to visualize better model training.\nmodel.fit(cached_train, epochs=3) Epoch 1/3 10/10 [==============================] - 31s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0016 - factorized_top_k/top_5_categorical_accuracy: 0.0113 - factorized_top_k/top_10_categorical_accuracy: 0.0246 - factorized_top_k/top_50_categorical_accuracy: 0.1124 - factorized_top_k/top_100_categorical_accuracy: 0.1992 - loss: 69688.8196 - regularization_loss: 0.0000e+00 - total_loss: 69688.8196 Epoch 2/3 10/10 [==============================] - 26s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0029 - factorized_top_k/top_5_categorical_accuracy: 0.0188 - factorized_top_k/top_10_categorical_accuracy: 0.0377 - factorized_top_k/top_50_categorical_accuracy: 0.1680 - factorized_top_k/top_100_categorical_accuracy: 0.2930 - loss: 67388.2720 - regularization_loss: 0.0000e+00 - total_loss: 67388.2720 Epoch 3/3 10/10 [==============================] - 27s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0034 - factorized_top_k/top_5_categorical_accuracy: 0.0228 - factorized_top_k/top_10_categorical_accuracy: 0.0461 - factorized_top_k/top_50_c Model evaluation And then finally, evaluate the model on the test set.\nmodel.evaluate(cached_test, return_dict=True) The performance on the test set is expected to be worse because its predictions are computed on data that has yet to be seen (there is always the risk of overfitting our model during training, so one must be careful).\n{\u0026#39;factorized_top_k/top_1_categorical_accuracy\u0026#39;: 0.0010000000474974513, \u0026#39;factorized_top_k/top_5_categorical_accuracy\u0026#39;: 0.008500000461935997, \u0026#39;factorized_top_k/top_10_categorical_accuracy\u0026#39;: 0.020349999889731407, \u0026#39;factorized_top_k/top_50_categorical_accuracy\u0026#39;: 0.12030000239610672, \u0026#39;factorized_top_k/top_100_categorical_accuracy\u0026#39;: 0.2313999980688095, \u0026#39;loss\u0026#39;: 28273.66796875, \u0026#39;regularization_loss\u0026#39;: 0, \u0026#39;total_loss\u0026#39;: 28273.66796875} Making predictions With the following code, we can make predictions for a given user.\n# Create a model that takes in raw query features, and index = tfrs.layers.factorized_top_k.BruteForce(model.user_model) # recommends movies out of the entire movies dataset. index.index_from_dataset( tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model))) ) # Get recommendations. _, titles = index(tf.constant([\u0026#34;42\u0026#34;])) %timeit _, titles = index(np.array([\u0026#34;42\u0026#34;]), k=3) print(f\u0026#34;Recommendations for user 42: {titles[0, :3]}\u0026#34;) Model serving To deploy a TensorFlow model, we only need the brute force layer run previously as well as the next piece of code.\n# Export the query model. with tempfile.TemporaryDirectory() as tmp: path = os.path.join(tmp, \u0026#34;model\u0026#34;) # Save the index. tf.saved_model.save(index, path) # Load it back; can also be done in TensorFlow Serving. loaded = tf.saved_model.load(path) # Pass a user id in, get top predicted movie titles back. scores, titles = loaded([\u0026#34;42\u0026#34;]) print(f\u0026#34;Recommendations: {titles[0][:3]}\u0026#34;) Efficient serving with ScaNN One of the drawbacks of using BruteForce is that our model will be much slower when making predictions. This could be problematic if latency is an essential requirement. We can use ScaNN to solve it.\nScaNN is a library from Google Research that indexes embeddings to allow them to be rapidly searched at inference time. It uses state-of-the-art vector compression techniques and can significantly outperform brute force search while losing minimal accuracy. To illustrate this idea, let’s create a vast number of candidates and compare the computational time between ScaNN and BruteForce.\nmodel.fit(cached_train, epochs=3) After running both, we can observe that ScaNN is much faster, and more importantly, the advantage of ScaNN over BruteForce will grow larger as the number of candidates increases.\nConclusions ","permalink":"https://andresruizc.github.io/andresr/posts/retrieval_tf/","summary":"Introduction to recommender systems Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations.","title":"Retrieval recommender systems with TensorFlow"},{"content":" Feeling ready to do something does not mean feeling certain that you\u0026rsquo;ll succeed, though of course that\u0026rsquo;s what you are hoping to do. Truly being ready means understanding what could go wrong – and having a plan to deal with it.\nChris Hadfield, An Astronaut\u0026rsquo;s Guide to Life on Earth\nEliud Kipchoge is considered the greatest marathon runner of all time. As with the most iconic athletes, the principles and values they embody are far more relevant than their achievements. In a speech at the Oxford Union in early 2018, Kipchoge said, \u0026ldquo;I believe in a philosophy that says to win is not important. To be successful is not even important. How to plan and prepare is critical and crucial. When you plan very well, then success can come on your way. Then winning can come on your way.\u0026rdquo;\nIt\u0026rsquo;s natural to feel inspired by him, but how can we prepare and become more competent? It depends on what we want to accomplish, but we can follow some general guidelines. In this post, we will explore the practice of thinking negatively.\nThink negatively In his book, An Astronaut\u0026rsquo;s Guide to Life on Earth, retired NASA astronaut Chris Hadfield explains that one of the best ways to prepare is to practice \u0026ldquo;The power of negative thinking.\u0026rdquo;\nAs he mentions in his book from his training program at NASA, \u0026ldquo;trainers in the space program specialize in devising bad-news scenarios for us to act out, over and over again, in increasingly elaborate simulations. We practice what we\u0026rsquo;ll do if there\u0026rsquo;s engine trouble, a computer meltdown, an explosion.\u0026rdquo; These simulations play a significant role in Chris Hadfield\u0026rsquo;s understanding of competence. He continues, \u0026ldquo;Being forced to confront the prospect of failure head-on-to study it, dissect it, tease apart all its components and consequences- really works. After a few years of doing that pretty much daily, you\u0026rsquo;ve forged the strongest possible armor to defend against fear: hard-won competence.\u0026rdquo;\nWhen the stakes are high, as with space exploration, nothing can be left to chance—every detail matters because the mission\u0026rsquo;s success and the astronaut\u0026rsquo;s survival depends on it—and these simulations made astronauts more competent.\nOne could argue that these simulations are not critical for competence because reality is very complex, and sooner than later, unexpected things happen. If that\u0026rsquo;s the case, what\u0026rsquo;s the point of devising bad news scenarios? The key is to see beyond the simulations themselves. The most interesting lesson was not about learning to deal with a computer meltdown or an explosion (which, by the way, was extremely important because it could be lifesaving) but the process that came after that.\nOnce the simulations were carried out, astronauts could analyze their decisions, listen to feedback, and look for areas of improvement. Maybe they performed poorly under pressure, or the crew could have cooperated more efficiently. Or perhaps these simulations made astronauts realize that making good decisions with limited information in a hostile environment is highly complicated. These lessons could be the key to competence; they apply to a specific-given simulation, and any challenge astronauts may encounter during their future missions. When the unexpected happens, not only in space exploration but also in life, knowing how to thrive under pressure or make good decisions with uncertainty could make a difference.\nWhen SpaceX, in partnership with NASA, was about to launch astronauts to the ISS for the first time, Elon Musk and NASA administrator Jim Bridenstine gave a short interview. When Elon was asked about the most challenging part of transitioning from cargo to crew, he said, \u0026ldquo;I told the SpaceX team that the mission reliability is the main priority right now … we\u0026rsquo;re just doing continuous ensuring reviews from now, nonstop, 24 hours a day until launch … we\u0026rsquo;re just looking for any possible action that could improve the probability of success no matter how small.\u0026rdquo; This obsession for detail is what it often takes to achieve competence and is also what NASA trainers were looking for when making up simulations for astronauts.\nThe practice of negative thinking is not specific to space exploration. For Marcus Aurelius, being a roman emperor was not easy. He faced war, disease, and even a full-scale civil war. In addition, he was surrounded by senators and military members who didn\u0026rsquo;t share his values, didn\u0026rsquo;t want him as emperor, and even wanted him dead.\nIn his Meditations, he tells himself: \u0026ldquo;When you wake up in the morning, tell yourself: The people I deal with today will be meddling, ungrateful, arrogant, dishonest, jealous, and surly. They are like this because they can\u0026rsquo;t tell good from evil \u0026hellip; And so, none of them can hurt me. No one can implicate me in ugliness. Nor can I feel angry at my relative or hate him.\u0026rdquo;\nNothing was more important for Marcus Aurelius than what the stoics call the four virtues: courage, discipline, wisdom, and temperance. In the face of chaos, it is straightforward to be carried away by negative emotions and respond irrationally. By planning his day and recognizing the problematic people he would encounter, he was in a better mindset to react rationally and calmly. He practiced negative visualization not because he was pessimistic but because he wanted to be better prepared for difficult situations.\nThe power of thinking negatively: a real-world example Consider a non-experienced runner training for a marathon. Marathons are demanding, both physically and mentally. They require mental toughness, discipline, good physical health, and hours and hours of training. Injuries are one of the most common drawbacks when training for a marathon; no one is immune to them.\nHow can we think negatively to avoid getting an injury? Before starting your training regime, consider the possibility of you getting injured. Assume the usual thing is to get hurt somewhere along the way, not the opposite. And once you have visualized how things could go wrong, act out and try to prevent them. To do so, you could start with the following questions:\n• What are the most common injuries among marathon runners?\n• What are some leading causes of injuries, and how can they be avoided?\n• How can we reduce the probability of getting injured?\n• What will you do if you get injured?\nAnd then, act. Study the most common injuries among marathon runners: shin splints, back pain, stress fractures, or runner\u0026rsquo;s knee. Second, try to understand why they happen; they may be due to a combination of different factors: improper running technique, lack of body mobility, insufficient muscle mass, inadequate running shoes, or overtraining, among others. Then, work on them one by one:\n• Add strengthening exercises to your routine to become stronger.\n• Add mobility and stretching exercises.\n• Schedule your workouts, leaving time to rest\n• Sleep well and eat healthily.\n• Make sure your running shoes are adequate.\n• Improve your running technique.\nYou may follow these rules and get injured, or not follow them and run a marathon injury-free. Does it mean you wasted your time? Not really. We can never be sure what will happen; some things remain outside our control. If you acted in a way that helped you move in the right direction, it\u0026rsquo;s okay.\nConclusion Sometimes visualizing how things could go wrong is complicated: we may feel fear, lack imagination, or prefer to assume things will always go our way. Don\u0026rsquo;t let all that stop you! The lessons you can learn from visualizing negative scenarios are definitively worth the effort. If astronauts, a roman emperor, or athletes can benefit from devising bad news scenarios and acting out, then the rest of us can. We all have room for improvement, and if there is something we can learn from these examples is that we are only limited by our willingness to improve.\n","permalink":"https://andresruizc.github.io/andresr/posts/first/","summary":"Feeling ready to do something does not mean feeling certain that you\u0026rsquo;ll succeed, though of course that\u0026rsquo;s what you are hoping to do. Truly being ready means understanding what could go wrong – and having a plan to deal with it.\nChris Hadfield, An Astronaut\u0026rsquo;s Guide to Life on Earth\nEliud Kipchoge is considered the greatest marathon runner of all time. As with the most iconic athletes, the principles and values they embody are far more relevant than their achievements.","title":"Thinking Negatively"},{"content":"Edith Eger \u0026ldquo;Our painful experiences aren\u0026rsquo;t a liability—they\u0026rsquo;re a gift. They give us perspective and meaning, an opportunity to find our unique purpose and our strength.\u0026rdquo;\n\u0026ldquo;It\u0026rsquo;s the first time I see that we have a choice: to pay attention to what we\u0026rsquo;ve lost or to pay attention to what we still have.\u0026rdquo; \u0026mdash; Edith Eger, The Choice: Embrace the Possible\nSuffering is universal. But victimhood is optional. There is a difference between victimization and victimhood. We are all likely to victimized in some way in the course of our lives. At some point we will suffer some kind of affliction or calamity or abuse, caused by circumstances or people or institutions over which we have little or no control. This is life. And this is victimization. It comes from outside. It\u0026rsquo;s the neighborhood bully, the boss who rages, the spouse who hits, the lover who cheats, the discriminatory law, the accident that lands you in the hospital.\nIn contrast, victimhood comes from the inside. No one can make you a victim but you. We become victims not because of what happens to us but when we choose to hold on to our victimization. We develop a victim\u0026rsquo;s mind \u0026ndash; a way of thinking and being that is rigid, blaming, pessimistic, stuck in the past, unforgiving, punitive, and without healthy limits or boundaries. We become our own jailors when we choose the confines of the victim\u0026rsquo;s mind.\nViktor Frankl “Everything can be taken from a man but one thing: the last of the human freedoms—to choose one’s attitude in any given set of circumstances, to choose one’s own way.”\n\u0026mdash; Viktor E. Frankl, Man\u0026rsquo;s Search for Meaning\n“When we are no longer able to change a situation, we are challenged to change ourselves.”\n“In some ways suffering ceases to be suffering at the moment it finds a meaning, such as the meaning of a sacrifice.”\n“The one thing you can’t take away from me is the way I choose to respond to what you do to me. The last of one’s freedoms is to choose one’s attitude in any given circumstance.”\nMarcus Aurelius “You have power over your mind - not outside events. Realize this, and you will find strength.”\n“Here is a rule to remember in future, when anything tempts you to feel bitter: not \u0026lsquo;This is misfortune,\u0026rsquo; but \u0026lsquo;To bear this worthily is good fortune.\u0026rsquo;\u0026quot;\n“If someone is able to show me that what I think or do is not right, I will happily change, for I seek the truth, by which no one was ever truly harmed. It is the person who continues in his self-deception and ignorance who is harmed.”\n“If you are distressed by anything external, the pain is not due to the thing itself, but to your estimate of it; and this you have the power to revoke at any moment.”\n“The best revenge is to be unlike him who performed the injury.”\n“Whenever you are about to find fault with someone, ask yourself the following question: What fault of mine most nearly resembles the one I am about to criticize?”\n“Do not act as if you were going to live ten thousand years. Death hangs over you. While you live, while it is in your power, be good.”\n“The impediment to action advances action. What stands in the way becomes the way.”\nPema Chödron \u0026ldquo;It’s not a terrible thing that we feel fear when faced with the unknown. It is part of being alive, something we all share. We react against the possibility of loneliness, of death, of not having anything to hold on to. Fear is a natural reaction to moving closer to the truth.\u0026rdquo;\n\u0026ldquo;Thinking that we can find some lasting pleasure and avoid pain is what in Buddhism is called samsara, a hopeless cycle that goes round and round endlessly and causes us to suffer greatly.\u0026rdquo;\n\u0026ldquo;The spiritual journey involves going beyond hope and fear, stepping into unknown territory, continually moving forward. The most important aspect of being on the spiritual path may be to just keep moving.\u0026rdquo;\nIn practicing meditation, we’re not trying to live up to some kind of ideal—quite the opposite. We’re just being with our experience, whatever it is. If our experience is that sometimes we have some kind of perspective, and sometimes we have none, then that’s our experience. If sometimes we can approach what scares us, and sometimes we absolutely can’t, then that’s our experience.\nThis very moment is the perfect teacher, and it’s always with us is really a most profound instruction. Just seeing what’s going on—that’s the teaching right there. We can be with what’s happening and not dissociate. Awakeness is found in our pleasure and our pain, our confusion and our wisdom, available in each moment of our weird, unfathomable, ordinary everyday lives.\n\u0026ldquo;The point of meditation is not to try to get rid of thoughts, but rather to see their true nature. Thoughts will run us around in circles if we buy into them, but really they are like dream images. They are like an illusion—not really all that solid. They are, as we say, just thinking.\u0026rdquo;\n\u0026ldquo;How we regard what arises in meditation is training for how we regard whatever arises in the rest of our lives.\u0026rdquo;\n\u0026ldquo;The first noble truth of the Buddha is that when we feel suffering, it doesn’t mean that something is wrong. What a relief. Finally somebody told the truth. Suffering is part of life, and we don’t have to feel it’s happening because we personally made the wrong move. In reality, however, when we feel suffering, we think that something is wrong. As long as we’re addicted to hope, we feel that we can tone our experience down or liven it up or change it somehow, and we continue to suffer a lot.\u0026rdquo;\nEverything that occurs is not only usable and workable but is actually the path itself. We can use everything that happens to us as the means for waking up. We can use everything that occurs—whether it’s our conflicting emotions and thoughts or our seemingly outer situation—to show us where we are asleep and how we can wake up completely, utterly, without reservations.\nThis is a very encouraging teaching, because it says that the source of wisdom is whatever is going to happen to us today. The source of wisdom is whatever is happening to us right at this very instant. If there’s any possibility for enlightenment, it’s right now, not at some future time. Now is the time.\n","permalink":"https://andresruizc.github.io/andresr/quotes/","summary":"Edith Eger \u0026ldquo;Our painful experiences aren\u0026rsquo;t a liability—they\u0026rsquo;re a gift. They give us perspective and meaning, an opportunity to find our unique purpose and our strength.\u0026rdquo;\n\u0026ldquo;It\u0026rsquo;s the first time I see that we have a choice: to pay attention to what we\u0026rsquo;ve lost or to pay attention to what we still have.\u0026rdquo; \u0026mdash; Edith Eger, The Choice: Embrace the Possible\nSuffering is universal. But victimhood is optional. There is a difference between victimization and victimhood.","title":"Favorite ideas"},{"content":"This is Andres.\n","permalink":"https://andresruizc.github.io/andresr/about/","summary":"This is Andres.","title":"Me"},{"content":"2023 11 Feb \u0026ndash; Retrieval recommender systems with TensorFlow\n04 Feb \u0026ndash; Thinking Negatively\n","permalink":"https://andresruizc.github.io/andresr/archive/","summary":"2023 11 Feb \u0026ndash; Retrieval recommender systems with TensorFlow\n04 Feb \u0026ndash; Thinking Negatively","title":"Posts Archive"},{"content":"Next The 48 Laws of Power by Robert Greene Build: An Unorthodox Guide to Making Things Worth Making by Tony Fadell Never Finished by David Goggins How to Think Like A Rocket Scientist by Ozan Varol The Hard Things About the Hard Things by Ben Horowitz The Courage to be Disliked by Ichiro Kishimi and Fumitake Koga The Ego is the Enemy by Ryan Holiday Stillness is the Key by Ryan Holiday The Lean Startup by Eric Ries Books read Atomic Habits by James Clear Be Water, My Friend by Shannon Lee Man\u0026rsquo;s Search for Meaning by Viktor Frankl The War of Art by Steven Pressfield The Midnigth Library by Matt Haig Meditations by Marcus Aurelius When Things Fall Apart by Pema Chödron How to meditate by Pema Chödron How to Think Like a Roman Emperor by Donald Robertson The Obstacle is the Way by Ryan Holiday The Art of a Good Life by William B.Irvine The Art of War by Sun Tzu The Proof is in the Plants by Simon Hill Sapiens by Yuval Noah Harari 21 lessons for the 21st century by Yuval Noah Harari Infinite Powers by Steven Strogatz Factfulness by Hans Rosling Siddhartha by Herman Hesse Farenheit 451 by Ray Bradbury 1984 by George Orwell Animal Farmm by George Orwell The Code Breakers by Walter Isaacson Steve Jobs by Walter Isaacson El día que se perdió la cordura By Javier Castillo The Gift: 12 lessons to save your life by Edith Eger The Choice: A true story of hope by Edith Eger Endurance:La prisión blanca:El legendario viaje de Shackleton al Polo Sur by Alfred Lansing Bajo el Aro by Pau Gasol El libro Negro del Emprendedor by Fernando Trias de Bes The Innovators by Walter Isaacson Iwoz. Steve Wozniak, De Genio De La Informática A Icono De Culto by Steve Wozniak An Austronauts Guide to Life on Earth Chris Hadfield The future of Humanity by Micho kaku Shoe Dog: A Memoir by the Creator of NIKE by Phil Knight The Psychology of Money by Morgan Housel Elon Musk: El empresario que anticipa el futuro by Ashley Vance El legado de Mandela: Quince enseñanzas sobre la vida, el amor y el valor by Richard Stengel Zero To One. Notes On Start Ups, Or How To Build The Future by Peter Thiel The Myth of Artificial Intelligence Erik J Larson ","permalink":"https://andresruizc.github.io/andresr/reading_list/","summary":"Next The 48 Laws of Power by Robert Greene Build: An Unorthodox Guide to Making Things Worth Making by Tony Fadell Never Finished by David Goggins How to Think Like A Rocket Scientist by Ozan Varol The Hard Things About the Hard Things by Ben Horowitz The Courage to be Disliked by Ichiro Kishimi and Fumitake Koga The Ego is the Enemy by Ryan Holiday Stillness is the Key by Ryan Holiday The Lean Startup by Eric Ries Books read Atomic Habits by James Clear Be Water, My Friend by Shannon Lee Man\u0026rsquo;s Search for Meaning by Viktor Frankl The War of Art by Steven Pressfield The Midnigth Library by Matt Haig Meditations by Marcus Aurelius When Things Fall Apart by Pema Chödron How to meditate by Pema Chödron How to Think Like a Roman Emperor by Donald Robertson The Obstacle is the Way by Ryan Holiday The Art of a Good Life by William B.","title":"Reading"}]