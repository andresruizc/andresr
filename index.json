[{"content":"Do difficult things every day. Step out of your comfort zone daily because that will callous your mind. Sounds good, so I researched cold showers and discovered they have been shown to have several benefits, including reduced inflammation, reduced stress levels, boosted energy, increased immunity, and enhanced focus, among others. And because I considered them as self-imposed torture, I decided to take cold showers every morning. Then, everything went differently than planned.\nThe first cold showers were manageable, but after a few days, they were getting progressively hotter and shorter. Mental discomfort increased, and the initial excitement I felt after discovering the benefits of cold showers morphed into self-doubt. My list of excuses to give up piled up, and as usually happens when creating new habits, I was about to quit. The cold shower of the next day would be the last.\nReaching our limit is not some kind of punishment. It\u0026rsquo;s actually a sign of health that, when we meet the place where we are about to die, we feel fear and trembling. A further sign of health is that we don\u0026rsquo;t become undone by fear and trembling, but we take it as a message that it\u0026rsquo;s time to stop struggling and look directly at what\u0026rsquo;s threatening us. Things like disappointment and anxiety are messengers telling us that we\u0026rsquo;re about to go into unknown territory.\nPema Chödron, When Things Fall Apart\nResistance I would define my experience before the cold shower as a state of agitation that biased me toward inaction through excuses, self-doubt, anxiety, and procrastination. A voice that prevented me from behaving differently, which is not specific to cold showers at all. It also appears when you decide to take on a new challenge but wonder if it is too audacious, when you need to work hard on a project and don\u0026rsquo;t feel like it, or when you consider asking someone out and fear of rejection stops you. So, you\u0026rsquo;d better learn to deal with it.\nSteven Pressfield, author of The War of Art, gives that voice a name: Resistance. He defines it as a force within you that prevents you from showing up as your best self. From working on a personal project. From doing things differently. From taking risks. According to Pressfield, Resistance is your greatest enemy, and the difference between amateurs and professionals in any endeavor boils down to beating it.\nHow it manifests is different for every person. For Pressfield, Resistance is \u0026ldquo;a voice in my head telling me why I can\u0026rsquo;t do something or why I should put it off for another day.\u0026rdquo; For others, it can be procrastination, self-doubt, fear of failure, inaction, ego, excuses, etc.\nAcknowledging the negative consequences of procrastination or fear of failure is an excellent first step, but that knowledge is not enough to beat Resistance. We need something more. We need to encounter that voice every day, and this is where cold showers help. As you may have guessed by now, this is not about cold showers. Cold showers are just a means to meet Resistance.\nOvercoming Resistance When things don\u0026rsquo;t go as expected, keeping the focus outside yourself is easier. In the beginning, I believed cold showers were the problem. That\u0026rsquo;s Resistance. But after weeks, I decided to change my approach. When Resistance appeared before and during the cold shower, it was all about observing. My thoughts, my reactions, and how I dealt with the situation. Some days I would complain, argue that it doesn\u0026rsquo;t make sense, or get paralyzed. Some days I was driven and found it easier. Other days I was not motivated at all, and it felt painful. It\u0026rsquo;s fine.\nThen I realized that observing, instead of running away from painful feelings, made everything easier. The problem was about my unrealistic expectations rather than with cold showers themselves. Dropping expectations, not judging, and just observing is challenging but leaves space for something more profound: the cold showers disappear, and it turns out that all you are doing is practicing meditation.\nIn practicing meditation, we\u0026rsquo;re not trying to live up to some kind of ideal—quite the opposite. We\u0026rsquo;re just being with our experience, whatever it is. If our experience is that sometimes we have some kind of perspective, and sometimes we have none, then that\u0026rsquo;s our experience. If sometimes we can approach what scares us, and sometimes we absolutely can\u0026rsquo;t, then that\u0026rsquo;s our experience.\nThis very moment is the perfect teacher, and it\u0026rsquo;s always with us is really a profound instruction. Just seeing what\u0026rsquo;s going on—that\u0026rsquo;s the teaching right there. We can be with what\u0026rsquo;s happening and not dissociate. Awakeness is found in our pleasure and our pain, our confusion and our wisdom, available in each moment of our weird, unfathomable, ordinary everyday lives.\nHow we regard what arises in meditation is training for how we regard whatever arises in the rest of our lives.\nPema Chödron, When Things Fall Apart\n","permalink":"https://andresruizc.github.io/andresr/posts/cold_showers/","summary":"Do difficult things every day. Step out of your comfort zone daily because that will callous your mind. Sounds good, so I researched cold showers and discovered they have been shown to have several benefits, including reduced inflammation, reduced stress levels, boosted energy, increased immunity, and enhanced focus, among others. And because I considered them as self-imposed torture, I decided to take cold showers every morning. Then, everything went differently than planned.","title":"Why I take cold showers"},{"content":"NOTE : This tutorial is a personal wrap-up of the TFX Pipeline Tutorial using the Penguin Dataset. The code used is identical, but I have added an introduction to TFX and several explanations for every section. The goal of this post to document my learning notes about TFX.\nWhat is TFX, and why is needed?\nMachine Learning is more than getting data, training models, and making predictions. A wide range of constraints and requirements need to be addressed for ML to add value in the real world:\nA highly available, scalable, and consistent infrastructure to store and process data as efficiently as possible.\nModel deployment. Not only do we need a recommender system that is able to make useful predictions, but we also need to deploy it so that users on their smartphones or web browser can benefit from it. When it comes to deploying models, here are some points to consider:\nAre we trying to optimize our application for latency or throughput? Does the application require our models to scale automatically to handle cyclic traffic requirements? Do we plan to compare models in production through A/B tests? Once our model is deployed, we need to monitor it to ensure it is working properly. In case it is not, there are several options. One of them could be to create a feedback loop used to continuously retrain the model and improve the confidence of future label predictions, also known as active learning.\nIn some areas, such as autonomous vehicles or healthcare, transparency about how ML models arrive at their predictions is critical to consumers and regulators who need to trust the model predictions if they will accept the decisions based on them. Tools for model governance are required.\nMLOps : a combinationof practices and tools that increase organizations\u0026rsquo; ability to deliver high-velocity applications and services.\nAnd finally, we need an end-to-end ML pipeline , an iterative procedure to automate and scale all the previous stages so that engineers can perform experiments until the desired performance is obtained.\nTFX is a set of libraries for building, tuning, deploying, and managing the production of ML models. It consists of a set of high-level APIs that can be used to perform data ingestion, data analysis and visualization, model development, and model deployment, among others. Our ML lifecycle doesn\u0026rsquo;t finish when models are deployed; there is a wide range of requirements, such as model governance, model auditing, or model tracking, and TFX offers tools for that.\nImage\nIt is used by top tech companies all around the globe across many different domains, including Spotify, Airbus, Gmail, and Twitter.\nFor example, Spotify offers a recommendation system that provides personalized music recommendations to its users based on different factors. One could think that this problem is solved just by creating a ML model with a good performance, but nothing is further from the truth. Spotify has millions of users, each one of them with hundreds of songs, audiobooks, or podcasts added to their libraries. The amount of data is vast, so scaling ML models requires a specific infrastructure. Here\u0026rsquo;s where TFX comes.\nA TFX can be used to create a pipeline that includes data ingestion, data visualization, data analysis, model training, model tuning, model deployment, model governance, and many more. Each of these steps can be implemented using TFX components (high-level APIs), and the result is an end-to-end pipeline that automates and scales all the previous stages in an efficient manner. Spotify also uses Google Cloud Platform and Kubeflow Pipelines.\nNow that we have introduced TFX, let\u0026rsquo;s code a simple pipeline.\nPackage installation and variables set up !pip install -U tfx !pip uninstall shapely -y # Temporal solution to avoid ImportError in Google Colab import tensorflow as tf print(\u0026#39;TensorFlow version: {}\u0026#39;.format(tf.__version__)) from tfx import v1 as tfx print(\u0026#39;TFX version: {}\u0026#39;.format(tfx.__version__)) import os from absl import logging PIPELINE_NAME = \u0026#34;penguin-simple\u0026#34; # Output directory to store artifacts generated from the pipeline. PIPELINE_ROOT = os.path.join(\u0026#39;pipelines\u0026#39;, PIPELINE_NAME) # Path to a SQLite DB file to use as an MLMD storage. METADATA_PATH = os.path.join(\u0026#39;metadata\u0026#39;, PIPELINE_NAME, \u0026#39;metadata.db\u0026#39;) # Output directory where created models from the pipeline will be exported. SERVING_MODEL_DIR = os.path.join(\u0026#39;serving_model\u0026#39;, PIPELINE_NAME) logging.set_verbosity(logging.INFO) Prepare data For this tutorial, we will be using the Palmer Penguins dataset, which contains size measurements for three penguins species observed on three different islands in the Palmer Archipelago, Antarctica. Apart from the species each penguin belongs to, it also contains four additional features: culmen_length_mm, culmen_depth_mm, flipper_length_mm, and body_mass_g.\nThe dataset can be downloaded through an URL, but because the TFX component ExampleGen reads from a directory, we need to copy the dataset to it.\nimport urllib.request import tempfile DATA_ROOT = tempfile.mkdtemp(prefix=\u0026#39;tfx-data\u0026#39;) # Create a temporary directory. _data_url = \u0026#39;https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv\u0026#39; _data_filepath = os.path.join(DATA_ROOT, \u0026#34;data.csv\u0026#34;) urllib.request.urlretrieve(_data_url, _data_filepath) !head {_data_filepath} species,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g 0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667 0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556 0,0.29818181818181805,0.5833333333333334,0.3898305084745763,0.1527777777777778 0,0.16727272727272732,0.7380952380952381,0.3559322033898305,0.20833333333333334 0,0.26181818181818167,0.892857142857143,0.3050847457627119,0.2638888888888889 0,0.24727272727272717,0.5595238095238096,0.15254237288135594,0.2569444444444444 0,0.25818181818181823,0.773809523809524,0.3898305084745763,0.5486111111111112 0,0.32727272727272727,0.5357142857142859,0.1694915254237288,0.1388888888888889 0,0.23636363636363636,0.9642857142857142,0.3220338983050847,0.3055555555555556 Pipeline components Our basic pipeline consists of five components: CsvExampleGen (ingest data files and converts them to TFX internal format), Trainer (trains an ML model), and Pusher (deploys the model on a serving infrastructure).\nWrite model training code The goal of the model is to classify a penguin based on the four attributes previously mentioned. There are several algorithms to solve this problem, such as KNN, Logistic regression, or SVMs, but we will code a very simple DNN.\nOne of the main differences between plain TensorFlow and TFX is that our model training code needs to be stored in a separate file, and some specific functions need to be included so that the Trainer component and the remaining components of the pipeline can properly work.\nThe first thing we need to do is to import the necessary libraries and define some variables.\nThe Trainer component requires a specific dataset schema, and a pipeline component but because we are working only with 4 features, we can manually define it by creating a feature spec.\nfrom typing import List from absl import logging import tensorflow as tf from tensorflow import keras from tensorflow_transform.tf_metadata import schema_utils from tfx import v1 as tfx from tfx_bsl.public import tfxio from tensorflow_metadata.proto.v0 import schema_pb2 _FEATURE_KEYS = [ \u0026#39;culmen_length_mm\u0026#39;, \u0026#39;culmen_depth_mm\u0026#39;, \u0026#39;flipper_length_mm\u0026#39;, \u0026#39;body_mass_g\u0026#39; ] _LABEL_KEY = \u0026#39;species\u0026#39; _TRAIN_BATCH_SIZE = 20 _EVAL_BATCH_SIZE = 10 _FEATURE_SPEC = { **{ feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for feature in _FEATURE_KEYS }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64) } We also need one function to generate the features and labels for model training. It will be used as the entry point for the Trainer component.\ndef _input_fn(file_pattern: List[str], data_accessor: tfx.components.DataAccessor, schema: schema_pb2.Schema, batch_size: int = 200) -\u0026gt; tf.data.Dataset: \u0026#34;\u0026#34;\u0026#34; Args: file_pattern: List of paths or patterns of input tfrecord files. data_accessor: DataAccessor for converting input to RecordBatch. schema: schema of the input data. batch_size: representing the number of consecutive elements of returned dataset to combine in a single batch Returns: A dataset that contains (features, indices) tuple where features is a dictionary of Tensors, and indices is a single Tensor of label indices. \u0026#34;\u0026#34;\u0026#34; return data_accessor.tf_dataset_factory( file_pattern, tfxio.TensorFlowDatasetOptions( batch_size=batch_size, label_key=_LABEL_KEY), schema=schema).repeat() We need another function to create the model using the Keras API. It uses a very simple DNN with two hidden layers, the Adam optimizer, the categorical cross entropy as loss function, and the accuracy as metric.\ndef _build_keras_model() -\u0026gt; tf.keras.Model: inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS] d = keras.layers.concatenate(inputs) for _ in range(2): d = keras.layers.Dense(8, activation=\u0026#39;relu\u0026#39;)(d) outputs = keras.layers.Dense(3)(d) model = keras.Model(inputs=inputs, outputs=outputs) model.compile( optimizer=keras.optimizers.Adam(1e-2), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.SparseCategoricalAccuracy()]) model.summary(print_fn=logging.info) return model And another function that will later be called by the TFX Trainer. It builds on the previous two. Finally, it saves the model.\ndef run_fn(fn_args: tfx.components.FnArgs): # This schema is usually either an output of SchemaGen or a manually-curated # version provided by pipeline author. A schema can also derived from TFT # graph if a Transform component is used. In the case when either is missing, # `schema_from_feature_spec` could be used to generate schema from very simple # feature_spec, but the schema returned would be very primitive. schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC) train_dataset = _input_fn( fn_args.train_files, fn_args.data_accessor, schema, batch_size=_TRAIN_BATCH_SIZE) eval_dataset = _input_fn( fn_args.eval_files, fn_args.data_accessor, schema, batch_size=_EVAL_BATCH_SIZE) model = _build_keras_model() model.fit( train_dataset, steps_per_epoch=fn_args.train_steps, validation_data=eval_dataset, validation_steps=fn_args.eval_steps) model.save(fn_args.serving_model_dir, save_format=\u0026#39;tf\u0026#39;) The following code cell puts everything together and writes it to a python file.\n_trainer_module_file = \u0026#39;penguin_trainer.py\u0026#39; %%writefile {_trainer_module_file} from typing import List from absl import logging import tensorflow as tf from tensorflow import keras from tensorflow_transform.tf_metadata import schema_utils from tfx import v1 as tfx from tfx_bsl.public import tfxio from tensorflow_metadata.proto.v0 import schema_pb2 _FEATURE_KEYS = [ \u0026#39;culmen_length_mm\u0026#39;, \u0026#39;culmen_depth_mm\u0026#39;, \u0026#39;flipper_length_mm\u0026#39;, \u0026#39;body_mass_g\u0026#39; ] _LABEL_KEY = \u0026#39;species\u0026#39; _TRAIN_BATCH_SIZE = 20 _EVAL_BATCH_SIZE = 10 _FEATURE_SPEC = { **{ feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for feature in _FEATURE_KEYS }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64) } def _input_fn(file_pattern: List[str], data_accessor: tfx.components.DataAccessor, schema: schema_pb2.Schema, batch_size: int = 200) -\u0026gt; tf.data.Dataset: return data_accessor.tf_dataset_factory( file_pattern, tfxio.TensorFlowDatasetOptions( batch_size=batch_size, label_key=_LABEL_KEY), schema=schema).repeat() def _build_keras_model() -\u0026gt; tf.keras.Model: inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS] d = keras.layers.concatenate(inputs) for _ in range(2): d = keras.layers.Dense(8, activation=\u0026#39;relu\u0026#39;)(d) outputs = keras.layers.Dense(3)(d) model = keras.Model(inputs=inputs, outputs=outputs) model.compile( optimizer=keras.optimizers.Adam(1e-2), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.SparseCategoricalAccuracy()]) model.summary(print_fn=logging.info) return model def run_fn(fn_args: tfx.components.FnArgs): train_dataset = _input_fn( fn_args.train_files, fn_args.data_accessor, schema, batch_size=_TRAIN_BATCH_SIZE) eval_dataset = _input_fn( fn_args.eval_files, fn_args.data_accessor, schema, batch_size=_EVAL_BATCH_SIZE) model = _build_keras_model() model.fit( train_dataset, steps_per_epoch=fn_args.train_steps, validation_data=eval_dataset, validation_steps=fn_args.eval_steps) model.save(fn_args.serving_model_dir, save_format=\u0026#39;tf\u0026#39;) Write a pipeline definition Now we can write a function to create the TFX pipeline. It consists of the three components previously mentioned (CsvExampleGen, Trainer, and Pusher).\ndef _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str, module_file: str, serving_model_dir: str, metadata_path: str) -\u0026gt; tfx.dsl.Pipeline: # Brings data into the pipeline. example_gen = tfx.components.CsvExampleGen(input_base=data_root) # Uses user-provided Python function that trains a model. trainer = tfx.components.Trainer( module_file=module_file, examples=example_gen.outputs[\u0026#39;examples\u0026#39;], train_args=tfx.proto.TrainArgs(num_steps=100), eval_args=tfx.proto.EvalArgs(num_steps=5)) # Pushes the model to a filesystem destination. pusher = tfx.components.Pusher( model=trainer.outputs[\u0026#39;model\u0026#39;], push_destination=tfx.proto.PushDestination( filesystem=tfx.proto.PushDestination.Filesystem( base_directory=serving_model_dir))) # Following three components will be included in the pipeline. components = [ example_gen, trainer, pusher, ] return tfx.dsl.Pipeline( pipeline_name=pipeline_name, pipeline_root=pipeline_root, metadata_connection_config=tfx.orchestration.metadata .sqlite_metadata_connection_config(metadata_path), components=components) Run the pipeline Finally, the full pipeline can be run with the following code. We could integrate this TFX pipeline with the Google Cloud AI Platform, which will be covered in another blog post.\ntfx.orchestration.LocalDagRunner().run( _create_pipeline( pipeline_name=PIPELINE_NAME, pipeline_root=PIPELINE_ROOT, data_root=DATA_ROOT, module_file=_trainer_module_file, serving_model_dir=SERVING_MODEL_DIR, metadata_path=METADATA_PATH)) We can check that the model trained is pushed by the Pusher to the directory specified in the beginning of this tutorial.\n!find {SERVING_MODEL_DIR} serving_model/penguin-simple serving_model/penguin-simple/1676215135 serving_model/penguin-simple/1676215135/keras_metadata.pb serving_model/penguin-simple/1676215135/variables serving_model/penguin-simple/1676215135/variables/variables.index serving_model/penguin-simple/1676215135/variables/variables.data-00000-of-00001 serving_model/penguin-simple/1676215135/assets serving_model/penguin-simple/1676215135/fingerprint.pb serving_model/penguin-simple/1676215135/saved_model.pb Conclusion and future lines of improvement In this tutorial we have first introduced TFX and took a look at how Spotify it to create personalized recommendations. Then, we created a basic Pipeline with three components: CsvExampleGen, Trainer, and Pusher.\nThis pipeline could be improved in several ways. First, more TFX components could be used:\nStatisticsGen to calculate statistics for the dataset. SchemaGen to examine the statistics and creates an initial data schema. Transform to perform high-quality feature engineering techniques. Evaluator component to compare our models with some predefined baseline. ","permalink":"https://andresruizc.github.io/andresr/posts/tfx_tutorial/","summary":"NOTE : This tutorial is a personal wrap-up of the TFX Pipeline Tutorial using the Penguin Dataset. The code used is identical, but I have added an introduction to TFX and several explanations for every section. The goal of this post to document my learning notes about TFX.\nWhat is TFX, and why is needed?\nMachine Learning is more than getting data, training models, and making predictions. A wide range of constraints and requirements need to be addressed for ML to add value in the real world:","title":"Introduction to TFX pipelines"},{"content":"Introduction to recommender systems Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations.\nHere are some types of recommender systems:\nCollaborative Filtering: Assumes people with similar preferences will have similar preferences in the future. If the person one like items A and B and person 2 likes item A, person two will likely want item B. Content-Based Filtering: Assumes people will prefer items like those they liked in the past. Suppose I bought a pair of shoes. Hybrid Recommender Systems: Combines both collaborative and content-based filtering. Matrix Factorization: Assumes hidden relationships in the data containing both users and items may exist. Association Rule Mining: Assumes items frequently purchased together are likely to interest the user. It is very common to work with vast amounts of data when building recommender systems, and computational and time requirements must be considered. That’s why real-world recommender systems are often divided into two stages:\nThe retrieval stage filters an initial set of potential candidates out of all possible ones. It must be computationally efficient because candidates are in the order of millions. The ranking stage uses the outputs of the retrieval model and selects the most likely candidates. In this tutorial, I will focus on the retrieval stage. Specifically, I will build a retrieval stage recommender system that predicts a set of movies a given user is likely to watch using TensorFlow and the MovieLens dataset.\nDataset The MovieLens dataset contains 100,000 ratings and 3,600 tags applied to 9,000 movies by 600 users, and it can be downloaded directly from TensorFlow Datasets. Both the datasets containing the movies and the rating will be downloaded, but we will only focus on the movies dataset.\nLet’s load both datasets and visualize the attributes of each observation.\n# Package installation !pip install -q tensorflow-recommenders !pip install -q --upgrade tensorflow-datasets !pip install -q scann # Loading libraries import os import pprint import tempfile from typing import Dict, Text import numpy as np import tensorflow as tf import tensorflow_datasets as tfds import tensorflow_recommenders as tfrs Let\u0026rsquo;s load both datasets\n# Ratings data. ratings = tfds.load(\u0026#34;movielens/100k-ratings\u0026#34;, split=\u0026#34;train\u0026#34;) # Features of all the available movies. movies = tfds.load(\u0026#34;movielens/100k-movies\u0026#34;, split=\u0026#34;train\u0026#34;) And visualize their attributes\nfor x in ratings.take(1).as_numpy_iterator(): pprint.pprint(x) for x in movies.take(1).as_numpy_iterator(): pprint.pprint(x) Rating dataset: {\u0026#39;bucketized_user_age\u0026#39;: 45.0, \u0026#39;movie_genres\u0026#39;: array([7]), \u0026#39;movie_id\u0026#39;: b\u0026#39;357\u0026#39;, \u0026#39;movie_title\u0026#39;: b\u0026#34;One Flew Over the Cuckoo\u0026#39;s Nest (1975)\u0026#34;, \u0026#39;raw_user_age\u0026#39;: 46.0, \u0026#39;timestamp\u0026#39;: 879024327, \u0026#39;user_gender\u0026#39;: True, \u0026#39;user_id\u0026#39;: b\u0026#39;138\u0026#39;, \u0026#39;user_occupation_label\u0026#39;: 4, \u0026#39;user_occupation_text\u0026#39;: b\u0026#39;doctor\u0026#39;, \u0026#39;user_rating\u0026#39;: 4.0, \u0026#39;user_zip_code\u0026#39;: b\u0026#39;53211\u0026#39;} Movie dataset: {\u0026#39;movie_genres\u0026#39;: array([4]), \u0026#39;movie_id\u0026#39;: b\u0026#39;1681\u0026#39;, \u0026#39;movie_title\u0026#39;: b\u0026#39;You So Crazy (1994)\u0026#39;} We only need the user_id and movie_title fields to build our retrieval stage. We are using TensorFlow for the data preprocessing stages because TensorFlow uses a specific data storage and message format that is targeted for ML workloads.Still, we could use other frameworks, such as Pandas, and then convert them back to a suitable form.\ndata storage and message format that is targeted for ML workloads.\nratings = ratings.map(lambda x: { \u0026#34;movie_title\u0026#34;: x[\u0026#34;movie_title\u0026#34;], \u0026#34;user_id\u0026#34;: x[\u0026#34;user_id\u0026#34;], }) movies = movies.map(lambda x: x[\u0026#34;movie_title\u0026#34;]) Training and test sets Why do we need to split our data into training and test sets? Because one of the most critical aspects of ML models is their ability to generalize well on unseen instances. If we were to use all our data to train the model and then evaluate model performance, we would not be capable of assessing how good our model is. Therefore, we need a training and test set.\nA good rule of thumb is to divide 80% for training and 20% for testing, and we’ll stick to that, but it depends on the specifics of your problem.\nThe strategy we use to split our data is also essential. If we are working with time series, we would have to divide based on timestamps. And the same applies to industrial recommender systems. In this tutorial, we won’t use any of that because our problem is simple.\ntf.random.set_seed(42) # to get reproducible partitions shuffled = ratings.shuffle(100000, seed=42, reshuffle_each_iteration=False) train = shuffled.take(80000) # 80% test = shuffled.skip(80000).take(20000) # 20% We also need to create lookup tables with unique values for our users and movie titles so that we can later use embeddings.\nmovie_titles = movies.batch(1000) user_ids = ratings.batch(1000000).map(lambda x: x[\u0026#34;user_id\u0026#34;]) unique_movie_titles = np.unique(np.concatenate(list(movie_titles))) unique_user_ids = np.unique(np.concatenate(list(user_ids))) Embeddings Now that we have a lookup table with unique values for our users and movie titles, we can use embeddings. Embeddings are dense representations of high-dimensional data, such as words or vocabulary. They can be understood as a lookup table that maps integer indices (e.g., user id and movie titles) to dense vectors.\nEmbeddings usually are initialized randomly, then trained using backpropagation along with the rest of the model parameters using gradient descent. Because they are trainable, they should improve during model training, hopefully in a way that allows us to cluster in high-dimensional space users and movie titles. The embeddings are learned such that the relationship between pairs of objects in the original space is preserved in the embedding space.\nThis is essential to our problem because recommending movies to specific users is like finding clusters of those users and movies. When we want to recommend movies to a new user, we just need to find the cluster to which this user belongs.\nOne hyperparameter we need to choose is the embedding dimension. The higher the dimension, the more accurate the embeddings, but the slower to train and more prone to overfitting embedding will be. 28 was selected as the embedding dimension.\nembedding_dimension = 28 One embedding will be created for the users and one for the movie titles. Then, they will be combined into a single model. Now, let’s talk about two crucial aspects of any deep learning model: metrics and loss functions.\nMetrics Metrics play a fundamental role in evaluating the performance of deep learning models. The concept of a metric is similar to the loss function, except that the metric is not used to update the model parameters during training, just for quantifying success and failure from a human perspective.\nSince we are using embeddings, one possible metric could be the following: take all your training data (user id and movie titles) and compute a score between a given pair (positive pair) and the rest of the observations (negative pair). Suppose the score of the positive pair is much higher than the score for the negative pairs. In that case, our embeddings can correctly create clusters in the high-dimensional space, which would imply more accurate recommendations.\nThis metric is implemented in TensorFlow as follows.\nmetrics = tfrs.metrics.FactorizedTopK( candidates=movies.batch(128).map(movie_model) ) Loss function Training a deep learning model could be defined as the iterative process of updating the model parameters so that the model predictions match the true labels. From a mathematical standpoint, making model predictions match the actual labels is equivalent to minimizing a loss function. Then, the model parameters are updated during training so that the predictions match the labels. Loss functions also help evaluate the performance of deep learning models.\nTFRS (TensorFlow Recommenders) provides several loss functions. Because we are solving a retrieval stage problem, the following will be used (our previously defined metrics must be passed as input)\ntask = tfrs.tasks.Retrieval( metrics=metrics ) Model implementation Let’s sum up what we have been doing so far.\nWe have converted users and movies into unique ID integer values. These ID values have created two embeddings for users and movie titles. (Embeddings could be defined as a representation of these ID values in a higher dimensional space that represents similarity) These two embeddings can be combined into a single model that outputs a score. We have defined a metric that uses this score to quantify our model\u0026rsquo;s performance. We have defined a loss function. Now, let’s put all the pieces together and implement our model (Explanations about the code are included with comments)\nclass NoBaseClassMovielensModel(tf.keras.Model): def __init__(self, user_model, movie_model): super().__init__() self.movie_model: tf.keras.Model = movie_model self.user_model: tf.keras.Model = user_model self.task: tf.keras.layers.Layer = task def train_step(self, features: Dict[Text, tf.Tensor]) -\u0026gt; tf.Tensor: with tf.GradientTape() as tape: user_embeddings = self.user_model(features[\u0026#34;user_id\u0026#34;]) positive_movie_embeddings = self.movie_model(features[\u0026#34;movie_title\u0026#34;]) loss = self.task(user_embeddings, positive_movie_embeddings) regularization_loss = sum(self.losses) total_loss = loss + regularization_loss gradients = tape.gradient(total_loss, self.trainable_variables) self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)) metrics = {metric.name: metric.result() for metric in self.metrics} metrics[\u0026#34;loss\u0026#34;] = loss metrics[\u0026#34;regularization_loss\u0026#34;] = regularization_loss metrics[\u0026#34;total_loss\u0026#34;] = total_loss return metrics def test_step(self, features: Dict[Text, tf.Tensor]) -\u0026gt; tf.Tensor: user_embeddings = self.user_model(features[\u0026#34;user_id\u0026#34;]) positive_movie_embeddings = self.movie_model(features[\u0026#34;movie_title\u0026#34;]) loss = self.task(user_embeddings, positive_movie_embeddings) regularization_loss = sum(self.losses) total_loss = loss + regularization_loss metrics = {metric.name: metric.result() for metric in self.metrics} metrics[\u0026#34;loss\u0026#34;] = loss metrics[\u0026#34;regularization_loss\u0026#34;] = regularization_loss metrics[\u0026#34;total_loss\u0026#34;] = total_loss return metrics Model fitting and evaluation We instantiate and compile the model. The optimizer and the learning rate determine how and how much the model parameters are updated during training. A standard choice is the Adam optimizer, with a learning rate of 0.1.\nmodel = MovielensModel(user_model, movie_model) model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1)) Then shuffle, batch, and cache the training and evaluation data. Gradient descent works better when data points are independently and identically distributed (IDD), and shuffling ensures this; our neural network is not batches of data, and the cache is used to speed up training.\ncached_train = train.shuffle(100000).batch(8192).cache() cached_test = test.batch(4096).cache() We train the model during three epochs. We can add a TensorBoard callback to the fit method to visualize better model training.\nmodel.fit(cached_train, epochs=5) Epoch 1/5 10/10 [==============================] - 36s 3s/step - factorized_top_k/top_1_categorical_accuracy: 8.1250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0072 - factorized_top_k/top_10_categorical_accuracy: 0.0165 - factorized_top_k/top_50_categorical_accuracy: 0.0916 - factorized_top_k/top_100_categorical_accuracy: 0.1671 - loss: 69862.4574 - regularization_loss: 0.0000e+00 - total_loss: 69862.4574 Epoch 2/5 10/10 [==============================] - 32s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0032 - factorized_top_k/top_5_categorical_accuracy: 0.0195 - factorized_top_k/top_10_categorical_accuracy: 0.0390 - factorized_top_k/top_50_categorical_accuracy: 0.1670 - factorized_top_k/top_100_categorical_accuracy: 0.2894 - loss: 67594.8033 - regularization_loss: 0.0000e+00 - total_loss: 67594.8033 Epoch 3/5 10/10 [==============================] - 29s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0036 - factorized_top_k/top_5_categorical_accuracy: 0.0232 - factorized_top_k/top_10_categorical_accuracy: 0.0450 - factorized_top_k/top_50_categorical_accuracy: 0.1848 - factorized_top_k/top_100_categorical_accuracy: 0.3121 - loss: 66420.7017 - regularization_loss: 0.0000e+00 - total_loss: 66420.7017 Epoch 4/5 10/10 [==============================] - 29s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0038 - factorized_top_k/top_5_categorical_accuracy: 0.0252 - factorized_top_k/top_10_categorical_accuracy: 0.0491 - factorized_top_k/top_50_categorical_accuracy: 0.1980 - factorized_top_k/top_100_categorical_accuracy: 0.3289 - loss: 65761.9844 - regularization_loss: 0.0000e+00 - total_loss: 65761.9844 Epoch 5/5 10/10 [==============================] - 29s 3s/step - factorized_top_k/top_1_categorical_accuracy: 0.0039 - factorized_top_k/top_5_categorical_accuracy: 0.0267 - factorized_top_k/top_10_categorical_accuracy: 0.05 Model evaluation And then finally, evaluate the model on the test set.\nmodel.evaluate(cached_test, return_dict=True) The performance on the test set is expected to be worse because its predictions are computed on data that has yet to be seen (there is always the risk of overfitting our model during training, so one must be careful).\n5/5 [==============================] - 10s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0010 - factorized_top_k/top_5_categorical_accuracy: 0.0070 - factorized_top_k/top_10_categorical_accuracy: 0.0161 - factorized_top_k/top_50_categorical_accuracy: 0.1103 - factorized_top_k/top_100_categorical_accuracy: 0.2186 - loss: 31147.1562 - regularization_loss: 0.0000e+00 - total_loss: 31147.1562 {\u0026#39;factorized_top_k/top_1_categorical_accuracy\u0026#39;: 0.0010499999625608325, \u0026#39;factorized_top_k/top_5_categorical_accuracy\u0026#39;: 0.007000000216066837, \u0026#39;factorized_top_k/top_10_categorical_accuracy\u0026#39;: 0.016100000590085983, \u0026#39;factorized_top_k/top_50_categorical_accuracy\u0026#39;: 0.11029999703168869, \u0026#39;factorized_top_k/top_100_categorical_accuracy\u0026#39;: 0.2186499983072281, \u0026#39;loss\u0026#39;: 28311.09375, \u0026#39;regularization_loss\u0026#39;: 0, \u0026#39;total_loss\u0026#39;: 28311.09375} Making predictions With the following code, we can make predictions for a given user.\nindex = tfrs.layers.factorized_top_k.BruteForce(model.user_model) index.index_from_dataset( tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model))) ) # Get recommendations for user 42 _, titles = index(tf.constant([\u0026#34;42\u0026#34;])) %timeit _, titles = index(np.array([\u0026#34;42\u0026#34;]), k=3) print(f\u0026#34;Recommendations for user 42: {titles[0, :3]}\u0026#34;) Model serving To deploy a TensorFlow model, we only need the brute force layer run previously as well as the next piece of code.\nwith tempfile.TemporaryDirectory() as tmp: path = os.path.join(tmp, \u0026#34;model\u0026#34;) # Save the index. tf.saved_model.save(index, path) # Load it back; can also be done in TensorFlow Serving. loaded = tf.saved_model.load(path) # Pass a user id in, get top predicted movie titles back. scores, titles = loaded([\u0026#34;42\u0026#34;]) print(f\u0026#34;Recommendations: {titles[0][:3]}\u0026#34;) Recommendations for user 42: [b\u0026#39;Rudy (1993)\u0026#39; b\u0026#39;Tombstone (1993)\u0026#39; b\u0026#39;Winnie the Pooh and the Blustery Day (1968)\u0026#39;] Efficient serving with ScaNN One of the drawbacks of using BruteForce is that our model will be much slower when making predictions. This could be problematic if latency is an essential requirement. We can use ScaNN to solve it.\nScaNN is a library from Google Research that indexes embeddings to allow them to be rapidly searched at inference time. It uses state-of-the-art vector compression techniques and can significantly outperform brute force search while losing minimal accuracy. To illustrate this idea, let’s create a vast number of candidates and compare the computational time between ScaNN and BruteForce.\nlots_of_movies = tf.data.Dataset.concatenate( movies.batch(8192), movies.batch(8192).repeat(1_000).map(lambda x: tf.zeros_like(x)) ) # We also add lots of dummy embeddings by randomly perturbing # the estimated embeddings for real movies. lots_of_movies_embeddings = tf.data.Dataset.concatenate( movies.batch(8192).map(model.movie_model), movies.batch(8192).repeat(1_000) .map(lambda x: model.movie_model(x)) .map(lambda x: x * tf.random.uniform(tf.shape(x))) ) BruteForce\nbrute_force_lots = tfrs.layers.factorized_top_k.BruteForce() brute_force_lots.index_from_dataset( tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings)) ) %timeit _, titles = brute_force_lots(model.user_model(np.array([\u0026#34;42\u0026#34;])), k=3) print(f\u0026#34;Top recommendations: {titles[0]}\u0026#34;) 41.2 ms ± 4.29 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Top recommendations: [b\u0026#39;Tombstone (1993)\u0026#39; b\u0026#34;Preacher\u0026#39;s Wife, The (1996)\u0026#34; b\u0026#39;American President, The (1995)\u0026#39; b\u0026#39;Dumbo (1941)\u0026#39; b\u0026#39;Rudy (1993)\u0026#39; b\u0026#34;Kid in King Arthur\u0026#39;s Court, A (1995)\u0026#34; b\u0026#39;Client, The (1994)\u0026#39; ScaNN\nscann = tfrs.layers.factorized_top_k.ScaNN( num_reordering_candidates=500, num_leaves_to_search=30 ) scann.index_from_dataset( tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings)) ) %timeit _, titles = scann(model.user_model(np.array([\u0026#34;42\u0026#34;])), k=3) print(f\u0026#34;Top recommendations: {titles[0]}\u0026#34;) 3.69 ms ± 261 µs per loop (mean ± std. dev. of 7 runs, 1 loop each) Top recommendations: [b\u0026#39;Tombstone (1993)\u0026#39; b\u0026#34;Preacher\u0026#39;s Wife, The (1996)\u0026#34; b\u0026#39;American President, The (1995)\u0026#39; b\u0026#39;Dumbo (1941)\u0026#39; b\u0026#39;Rudy (1993)\u0026#39; b\u0026#34;Kid in King Arthur\u0026#39;s Court, A (1995)\u0026#34; b\u0026#39;Client, The (1994)\u0026#39; After running both, we can observe that while both BruteForce and ScaNN obtain the same top 7 predictions predictions, but ScaNN is much faster. We improve inference time without an apparent loss of accuracy. Nice!\nIn addition, the advantage of ScaNN over BruteForce will grow larger as the number of candidates increases.\nConclusions ","permalink":"https://andresruizc.github.io/andresr/posts/retrieval_tf/","summary":"Introduction to recommender systems Some of the most exciting features that big companies like Amazon, Netflix, and Spotify offer rely on recommender systems. Much useful information can be collected whenever users navigate the Amazon website, listen to a playlist on Spotify, or scroll through the Netflix catalog. This data includes user behavior, item attributes, or interactions between users and is what machine learning recommender systems use to make personalized recommendations.","title":"Retrieval stage recommender systems with TensorFlow"},{"content":"You who are passing by\nI beg you\nDo something\nLearn a dance step\nSomething to justify your existence\nSomething that gives you the right\nTo be dressed in your skin in your body\nLearn to walk and to laugh\nBecause it would be too senseless\nAfter all\nFor many to have died\nWhile you live\nDoing nothing with your life\nAuschwitz survivor Charlotte\n","permalink":"https://andresruizc.github.io/andresr/posts/charlotte_poem/","summary":"You who are passing by\nI beg you\nDo something\nLearn a dance step\nSomething to justify your existence\nSomething that gives you the right\nTo be dressed in your skin in your body\nLearn to walk and to laugh\nBecause it would be too senseless\nAfter all\nFor many to have died\nWhile you live\nDoing nothing with your life\nAuschwitz survivor Charlotte","title":"You who are passing by"},{"content":" Feeling ready to do something does not mean feeling certain that you\u0026rsquo;ll succeed, though of course that\u0026rsquo;s what you are hoping to do. Truly being ready means understanding what could go wrong – and having a plan to deal with it.\nChris Hadfield, An Astronaut\u0026rsquo;s Guide to Life on Earth\nEliud Kipchoge is considered the greatest marathon runner of all time. As with the most iconic athletes, the principles and values they embody are far more relevant than their achievements. In a speech at the Oxford Union in early 2018, Kipchoge said, \u0026ldquo;I believe in a philosophy that says to win is not important. To be successful is not even important. How to plan and prepare is critical and crucial. When you plan very well, then success can come on your way. Then winning can come on your way.\u0026rdquo;\nIt\u0026rsquo;s natural to feel inspired by him, but how can we prepare and become more competent? It depends on what we want to accomplish, but we can follow some general guidelines. In this post, we will explore the practice of thinking negatively.\nThink negatively In his book, An Astronaut\u0026rsquo;s Guide to Life on Earth, retired NASA astronaut Chris Hadfield explains that one of the best ways to prepare is to practice \u0026ldquo;The power of negative thinking.\u0026rdquo;\nAs he mentions in his book from his training program at NASA, \u0026ldquo;trainers in the space program specialize in devising bad-news scenarios for us to act out, over and over again, in increasingly elaborate simulations. We practice what we\u0026rsquo;ll do if there\u0026rsquo;s engine trouble, a computer meltdown, an explosion.\u0026rdquo; These simulations play a significant role in Chris Hadfield\u0026rsquo;s understanding of competence. He continues, \u0026ldquo;Being forced to confront the prospect of failure head-on-to study it, dissect it, tease apart all its components and consequences- really works. After a few years of doing that pretty much daily, you\u0026rsquo;ve forged the strongest possible armor to defend against fear: hard-won competence.\u0026rdquo;\nWhen the stakes are high, as with space exploration, nothing can be left to chance — every detail matters because the mission\u0026rsquo;s success and the astronaut\u0026rsquo;s survival depends on it — and these simulations made astronauts more competent.\nOne could argue that these simulations are not critical for competence because reality is very complex, and sooner than later, unexpected things happen. If that\u0026rsquo;s the case, what\u0026rsquo;s the point of devising bad news scenarios? The key is to see beyond the simulations themselves. The most interesting lesson was not about learning to deal with a computer meltdown or an explosion (which, by the way, was extremely important because it could be lifesaving) but the learning process that came after that.\nOnce the simulations were carried out, astronauts could analyze their decisions, listen to feedback, and look for areas of improvement. Maybe they performed poorly under pressure, or the crew could have cooperated more efficiently. Or perhaps these simulations made astronauts realize that making good decisions with limited information in a hostile environment is highly complicated. These lessons could be the key to competence; they apply to a specific simulation, and any challenge astronauts may encounter during their future missions. When the unexpected happens, not only in space exploration but also in life, knowing how to thrive under pressure or make good decisions with uncertainty could make a difference.\nThinking negatively also teaches you another critical lesson: the obsession with detail. When simulating bad scenarios, you are constantly looking for potential problems, which can also be considered areas of improvement. This mindset is what it often takes to achieve competence in any endeavor and is what NASA trainers were seeking when making up simulations for astronauts.\nThe practice of negative thinking is not specific to astronauts. For Marcus Aurelius, being a roman emperor was not easy. He faced war, disease, and even a full-scale civil war. In addition, he was surrounded by senators and military members who didn\u0026rsquo;t share his values, didn\u0026rsquo;t want him as emperor, and even wanted him dead.\nIn his Meditations, he tells himself: \u0026ldquo;When you wake up in the morning, tell yourself: The people I deal with today will be meddling, ungrateful, arrogant, dishonest, jealous, and surly. They are like this because they can\u0026rsquo;t tell good from evil … And so, none of them can hurt me. No one can implicate me in ugliness. Nor can I feel angry at my relative or hate him.\u0026rdquo;\nNothing was more important for Marcus Aurelius than what the stoics call the four virtues: courage, discipline, wisdom, and temperance. In the face of chaos, it is straightforward to be carried away by negative emotions and respond irrationally. By planning his day and recognizing the problematic people he would encounter, he was in a better mindset to react rationally and calmly. He practiced negative visualization not because he was pessimistic but because he wanted to be better prepared for difficult situations.\nThinking negatively: running a marathon Consider a non-experienced runner training for a marathon. Marathons are demanding, both physically and mentally. They require mental toughness, discipline, good physical health, and hours and hours of training. Injuries are one of the most common drawbacks when training for a marathon; no one is immune to them.\nHow can we think negatively to avoid getting an injury? Before starting your training regime, consider the possibility of you getting injured. Assume the usual thing is to get hurt somewhere along the way, not the opposite. And once you have visualized how things could go wrong, act out and try to prevent them. To do so, you could start with the following questions:\n• What are the most common injuries among marathon runners?\n• What are some leading causes of injuries, and how can they be avoided?\n• How can we reduce the probability of getting injured?\n• What will you do if you get injured?\nAnd then, act. Study the most common injuries among marathon runners: shin splints, back pain, stress fractures, or Runner\u0026rsquo;s knee. Second, try to understand why they happen; they may be due to a combination of different factors: improper running technique, lack of body mobility, insufficient muscle mass, inadequate running shoes, or overtraining, among others. Then, work on them one by one:\n• Add strengthening exercises to your routine to become stronger.\n• Add mobility and stretching exercises.\n• Schedule your workouts, leaving time to rest\n• Sleep well and eat healthily.\n• Make sure your running shoes are adequate.\n• Improve your running technique.\nYou may follow these rules and get injured, or not follow them and run a marathon injury-free. Does it mean you wasted your time? Not really. We can never be sure what will happen; some things remain outside our control. If you acted in a way that helped you move in the right direction, it\u0026rsquo;s okay.\nConclusion Sometimes visualizing how things could go wrong is complicated: we may feel fear, lack imagination, or prefer to assume things will always go our way. Don\u0026rsquo;t let all that stop you! The lessons you can learn from visualizing negative scenarios are definitively worth the effort. If astronauts, a roman emperor, or athletes can benefit from devising bad news scenarios and acting out, then the rest of us can. We all have room for improvement, and if there is something we can learn from these examples is that we are only limited by our willingness to improve.\n","permalink":"https://andresruizc.github.io/andresr/posts/think_negatively/","summary":"Feeling ready to do something does not mean feeling certain that you\u0026rsquo;ll succeed, though of course that\u0026rsquo;s what you are hoping to do. Truly being ready means understanding what could go wrong – and having a plan to deal with it.\nChris Hadfield, An Astronaut\u0026rsquo;s Guide to Life on Earth\nEliud Kipchoge is considered the greatest marathon runner of all time. As with the most iconic athletes, the principles and values they embody are far more relevant than their achievements.","title":"Thinking Negatively"},{"content":"Edith Eger \u0026ldquo;Our painful experiences aren\u0026rsquo;t a liability—they\u0026rsquo;re a gift. They give us perspective and meaning, an opportunity to find our unique purpose and our strength.\u0026rdquo;\n\u0026ldquo;It\u0026rsquo;s the first time I see that we have a choice: to pay attention to what we\u0026rsquo;ve lost or to pay attention to what we still have.\u0026rdquo;\n\u0026ldquo;Survivors don\u0026rsquo;t have time to ask, \u0026lsquo;Why me?\u0026rsquo; For survivors, the only relevant question is, \u0026lsquo;What now?\u0026rsquo;\u0026rdquo;\n“A good definition of being a victim is when you keep the focus outside yourself, when you look outside yourself for someone to blame for your present circumstances, or to determine your purpose, fate, or worth.”\nSuffering is universal. But victimhood is optional. There is a difference between victimization and victimhood. We are all likely to victimized in some way in the course of our lives. At some point we will suffer some kind of affliction or calamity or abuse, caused by circumstances or people or institutions over which we have little or no control. This is life. And this is victimization. It comes from outside. It\u0026rsquo;s the neighborhood bully, the boss who rages, the spouse who hits, the lover who cheats, the discriminatory law, the accident that lands you in the hospital.\nIn contrast, victimhood comes from the inside. No one can make you a victim but you. We become victims not because of what happens to us but when we choose to hold on to our victimization. We develop a victim\u0026rsquo;s mind \u0026ndash; a way of thinking and being that is rigid, blaming, pessimistic, stuck in the past, unforgiving, punitive, and without healthy limits or boundaries. We become our own jailors when we choose the confines of the victim\u0026rsquo;s mind.\nViktor Frankl “Everything can be taken from a man but one thing: the last of the human freedoms—to choose one’s attitude in any given set of circumstances, to choose one’s own way.”\n“When we are no longer able to change a situation, we are challenged to change ourselves.”\n“In some ways suffering ceases to be suffering at the moment it finds a meaning, such as the meaning of a sacrifice.”\n“The one thing you can’t take away from me is the way I choose to respond to what you do to me. The last of one’s freedoms is to choose one’s attitude in any given circumstance.”\nMarcus Aurelius “You have power over your mind - not outside events. Realize this, and you will find strength.”\n“Here is a rule to remember in future, when anything tempts you to feel bitter: not \u0026lsquo;This is misfortune,\u0026rsquo; but \u0026lsquo;To bear this worthily is good fortune.\u0026rsquo;\u0026quot;\n“If someone is able to show me that what I think or do is not right, I will happily change, for I seek the truth, by which no one was ever truly harmed. It is the person who continues in his self-deception and ignorance who is harmed.”\n“If you are distressed by anything external, the pain is not due to the thing itself, but to your estimate of it; and this you have the power to revoke at any moment.”\n“The best revenge is to be unlike him who performed the injury.”\n“Whenever you are about to find fault with someone, ask yourself the following question: What fault of mine most nearly resembles the one I am about to criticize?”\n“Do not act as if you were going to live ten thousand years. Death hangs over you. While you live, while it is in your power, be good.”\n“The impediment to action advances action. What stands in the way becomes the way.”\nPema Chödron \u0026ldquo;It’s not a terrible thing that we feel fear when faced with the unknown. It is part of being alive, something we all share. We react against the possibility of loneliness, of death, of not having anything to hold on to. Fear is a natural reaction to moving closer to the truth.\u0026rdquo;\n\u0026ldquo;Thinking that we can find some lasting pleasure and avoid pain is what in Buddhism is called samsara, a hopeless cycle that goes round and round endlessly and causes us to suffer greatly.\u0026rdquo;\n\u0026ldquo;The spiritual journey involves going beyond hope and fear, stepping into unknown territory, continually moving forward. The most important aspect of being on the spiritual path may be to just keep moving.\u0026rdquo;\nIn practicing meditation, we’re not trying to live up to some kind of ideal—quite the opposite. We’re just being with our experience, whatever it is. If our experience is that sometimes we have some kind of perspective, and sometimes we have none, then that’s our experience. If sometimes we can approach what scares us, and sometimes we absolutely can’t, then that’s our experience.\nThis very moment is the perfect teacher, and it’s always with us is really a most profound instruction. Just seeing what’s going on—that’s the teaching right there. We can be with what’s happening and not dissociate. Awakeness is found in our pleasure and our pain, our confusion and our wisdom, available in each moment of our weird, unfathomable, ordinary everyday lives.\n\u0026ldquo;The point of meditation is not to try to get rid of thoughts, but rather to see their true nature. Thoughts will run us around in circles if we buy into them, but really they are like dream images. They are like an illusion—not really all that solid. They are, as we say, just thinking.\u0026rdquo;\n\u0026ldquo;How we regard what arises in meditation is training for how we regard whatever arises in the rest of our lives.\u0026rdquo;\nEverything that occurs is not only usable and workable but is actually the path itself. We can use everything that happens to us as the means for waking up. We can use everything that occurs—whether it’s our conflicting emotions and thoughts or our seemingly outer situation—to show us where we are asleep and how we can wake up completely, utterly, without reservations.\nThis is a very encouraging teaching, because it says that the source of wisdom is whatever is going to happen to us today. The source of wisdom is whatever is happening to us right at this very instant. If there’s any possibility for enlightenment, it’s right now, not at some future time. Now is the time.\n","permalink":"https://andresruizc.github.io/andresr/quotes/","summary":"Edith Eger \u0026ldquo;Our painful experiences aren\u0026rsquo;t a liability—they\u0026rsquo;re a gift. They give us perspective and meaning, an opportunity to find our unique purpose and our strength.\u0026rdquo;\n\u0026ldquo;It\u0026rsquo;s the first time I see that we have a choice: to pay attention to what we\u0026rsquo;ve lost or to pay attention to what we still have.\u0026rdquo;\n\u0026ldquo;Survivors don\u0026rsquo;t have time to ask, \u0026lsquo;Why me?\u0026rsquo; For survivors, the only relevant question is, \u0026lsquo;What now?\u0026rsquo;\u0026rdquo;","title":"Favorite ideas"},{"content":"","permalink":"https://andresruizc.github.io/andresr/ms/","summary":"","title":"Machine Learning"},{"content":"This is Andres.\n","permalink":"https://andresruizc.github.io/andresr/about/","summary":"This is Andres.","title":"Me"},{"content":"2023 13 Feb \u0026ndash; Why I take cold showers\n12 Feb \u0026ndash; Introduction to TFX pipelines\n11 Feb \u0026ndash; Retrieval stage recommender systems with TensorFlow\n07 Feb \u0026ndash; You who are passing by\n04 Feb \u0026ndash; Thinking Negatively\n","permalink":"https://andresruizc.github.io/andresr/archive/","summary":"2023 13 Feb \u0026ndash; Why I take cold showers\n12 Feb \u0026ndash; Introduction to TFX pipelines\n11 Feb \u0026ndash; Retrieval stage recommender systems with TensorFlow\n07 Feb \u0026ndash; You who are passing by\n04 Feb \u0026ndash; Thinking Negatively","title":"Posts Archive"},{"content":"Next Build: An Unorthodox Guide to Making Things Worth Making by Tony Fadell Never Finished by David Goggins How to Think Like A Rocket Scientist by Ozan Varol The Courage to be Disliked by Ichiro Kishimi and Fumitake Koga Ego is the Enemy by Ryan Holiday Homo Deus by Yuval Noah Harari El color púrpura by Alice Walker Books read Atomic Habits by James Clear Be Water, My Friend by Shannon Lee Can\u0026rsquo;t Hurt Me by David Goggins Man\u0026rsquo;s Search for Meaning by Viktor Frankl The War of Art by Steven Pressfield The Midnigth Library by Matt Haig Meditations by Marcus Aurelius When Things Fall Apart by Pema Chödron How to meditate by Pema Chödron How to Think Like a Roman Emperor by Donald Robertson The Obstacle is the Way by Ryan Holiday The Art of a Good Life by William B.Irvine How Life Imitates Chess by Garry Kasparov AI Superpowers. China, Silicon Valley and the new world order by Kai-Fu Lee Seven and a half lessons about the brain by Lisa Feldman Barrett The Art of War by Sun Tzu The Proof is in the Plants by Simon Hill Sapiens by Yuval Noah Harari 21 lessons for the 21st century by Yuval Noah Harari Infinite Powers by Steven Strogatz Factfulness by Hans Rosling Siddhartha by Herman Hesse Farenheit 451 by Ray Bradbury 1984 by George Orwell Animal Farmm by George Orwell The Code Breakers by Walter Isaacson Steve Jobs by Walter Isaacson El día que se perdió la cordura By Javier Castillo The Gift: 12 lessons to save your life by Edith Eger The Choice: A true story of hope by Edith Eger Endurance: La prisión blanca: El legendario viaje de Shackleton al Polo Sur by Alfred Lansing Bajo el Aro by Pau Gasol El libro Negro del Emprendedor by Fernando Trias de Bes The Innovators by Walter Isaacson Iwoz. Steve Wozniak, De Genio De La Informática A Icono De Culto by Steve Wozniak An Astronauts Guide to Life on Earth Chris Hadfield The future of Humanity by Micho kaku Shoe Dog: A Memoir by the Creator of NIKE by Phil Knight The Psychology of Money by Morgan Housel Elon Musk: El empresario que anticipa el futuro by Ashley Vance El legado de Mandela: Quince enseñanzas sobre la vida, el amor y el valor by Richard Stengel Zero To One. Notes On Start Ups, Or How To Build The Future by Peter Thiel The Myth of Artificial Intelligence Erik J Larson ","permalink":"https://andresruizc.github.io/andresr/reading_list/","summary":"Next Build: An Unorthodox Guide to Making Things Worth Making by Tony Fadell Never Finished by David Goggins How to Think Like A Rocket Scientist by Ozan Varol The Courage to be Disliked by Ichiro Kishimi and Fumitake Koga Ego is the Enemy by Ryan Holiday Homo Deus by Yuval Noah Harari El color púrpura by Alice Walker Books read Atomic Habits by James Clear Be Water, My Friend by Shannon Lee Can\u0026rsquo;t Hurt Me by David Goggins Man\u0026rsquo;s Search for Meaning by Viktor Frankl The War of Art by Steven Pressfield The Midnigth Library by Matt Haig Meditations by Marcus Aurelius When Things Fall Apart by Pema Chödron How to meditate by Pema Chödron How to Think Like a Roman Emperor by Donald Robertson The Obstacle is the Way by Ryan Holiday The Art of a Good Life by William B.","title":"Reading"}]